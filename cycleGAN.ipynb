{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1atBwz4DUxQaR3-9DfpXynSAOtv27iQLp",
      "authorship_tag": "ABX9TyNvZ2f3ZV2FedcRMAfLXF2B",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Disha-Sikka/SAR-to-EO-CycleGAN/blob/main/cycleGAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5O0WzZ17LDUv",
        "outputId": "497526d5-adf8-43aa-f781-1f9789422185"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Copying Extracted files in Colab's Local Disk"
      ],
      "metadata": {
        "id": "acjJOSnyex6Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "import os\n",
        "\n",
        "# store location of files in drive. So, that we can copy them\n",
        "drive_path_s1 = '/content/drive/MyDrive/CycleGAN/winter_s1'\n",
        "drive_path_s2 = '/content/drive/MyDrive/CycleGAN/winter_s2'\n",
        "\n",
        "# store location of colab's paths. Where you want to copy files\n",
        "colab_path_s1 = '/content/ROIs2017_winter_s1.tar.gz'\n",
        "colab_path_s2 = '/content/ROIs2017_winter_s2.tar.gz'\n",
        "\n",
        "print(\"Copying ROIs2017_winter_s1.tar.gz from Drive to Colab local disk...\")\n",
        "if os.path.exists(drive_path_s1):\n",
        "    shutil.copytree(drive_path_s1, colab_path_s1) # copytree --> is used to copy a folder while copy is used to cop a zip file\n",
        "    print(\"S1 file copied.\")\n",
        "else:\n",
        "    print(\"Wrong Path\")\n",
        "\n",
        "print(\"Copying ROIs2017_winter_s2.tar.gz from Drive to Colab local disk...\")\n",
        "if os.path.exists(drive_path_s2):\n",
        "    shutil.copytree(drive_path_s2, colab_path_s2)\n",
        "    print(\"S2 file copied.\")\n",
        "else:\n",
        "    print(\"Wrong Path\")\n",
        "\n",
        "print(\"Copying complete.\")"
      ],
      "metadata": {
        "id": "q1CywYH9ahp-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "231b753d-c8a7-4484-de9f-d19517c125e5"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Copying ROIs2017_winter_s1.tar.gz from Drive to Colab local disk...\n",
            "S1 file copied.\n",
            "Copying ROIs2017_winter_s2.tar.gz from Drive to Colab local disk...\n",
            "S2 file copied.\n",
            "Copying complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importing Libraries"
      ],
      "metadata": {
        "id": "lt6EuG5EpP59"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "import random\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.utils as vutils\n",
        "\n",
        "from skimage.metrics import structural_similarity as ssim\n",
        "from skimage.metrics import peak_signal_noise_ratio as psnr"
      ],
      "metadata": {
        "id": "R0gKf_UlpM8i"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Configuration\n",
        "\n",
        "- For defining the parameters. So, that we can easily change them when we want."
      ],
      "metadata": {
        "id": "vxeNC3Lqucof"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Config:\n",
        "    # Data paths\n",
        "    SAR_DIR = '/content/ROIs2017_winter_s1.tar.gz'\n",
        "    EO_DIR = '/content/ROIs2017_winter_s2.tar.gz'\n",
        "\n",
        "    # Model parameters\n",
        "    INPUT_NC = 2 # Number of input channels for SAR (Sentinel-1 GRD usually has 2: VV, VH)\n",
        "    NGF = 64 # Number of generator filters in the first conv layer\n",
        "    NDF = 64 # Number of discriminator filters in the first conv layer\n",
        "    N_RESNET_BLOCKS = 6 # Number of ResNet blocks in the generator\n",
        "\n",
        "    # Training parameters\n",
        "    BATCH_SIZE = 1 # CycleGAN typically uses batch size 1\n",
        "    NUM_EPOCHS = 20\n",
        "    LR = 0.0002 # Learning rate\n",
        "    BETA1 = 0.5 # Adam optimizer beta1\n",
        "    LAMBDA_CYCLE = 10.0 # Weight for cycle consistency loss\n",
        "    LAMBDA_IDENTITY = 5.0 # Weight for identity mapping loss helps stabilize\n",
        "\n",
        "    # Image parameters\n",
        "    IMAGE_SIZE = 256\n",
        "    NUM_WORKERS = 4\n",
        "\n",
        "    # Output and logging\n",
        "    # Save outputs and checkpoints to Google Drive for persistence across sessions\n",
        "    OUTPUT_BASE_DIR = '/content/drive/MyDrive/CycleGAN/SAR_EO_Project_Outputs' # Base directory in Drive\n",
        "    OUTPUT_DIR = os.path.join(OUTPUT_BASE_DIR, 'output_cyclegan') # Specific output for images\n",
        "    CHECKPOINT_DIR = os.path.join(OUTPUT_BASE_DIR, 'checkpoints_cyclegan') # Specific output for models\n",
        "\n",
        "    SAVE_EPOCH_FREQ = 5 # Save model checkpoints every N epochs\n",
        "    PRINT_FREQ = 1 # Print training loss every N batches\n",
        "\n",
        "    # Device\n",
        "    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    # EO Band Configurations (Sentinel-2 bands)\n",
        "    # B1 (Coastal Aerosol), B2 (Blue), B3 (Green), B4 (Red), B5 (Red Edge 1),\n",
        "    # B6 (Red Edge 2), B7 (Red Edge 3), B8 (NIR), B8A (NIR Narrow), B9 (Water Vapour),\n",
        "    # B10 (SWIR - Cirrus), B11 (SWIR 1), B12 (SWIR 2)\n",
        "    EO_BAND_CONFIGS = {\n",
        "        \"RGB\": [4, 3, 2], # B4, B3, B2 (Red, Green, Blue)\n",
        "        \"NIR_SWIR_RedEdge\": [8, 11, 5], # B8, B11, B5 (NIR, SWIR1, Red Edge 1)\n",
        "        \"RGB_NIR\": [4, 3, 2, 8] # B4, B3, B2, B8 (Red, Green, Blue, NIR)\n",
        "    }\n",
        "\n",
        "    CURRENT_EO_CONFIG_NAME = \"RGB\"\n",
        "    OUTPUT_NC = len(EO_BAND_CONFIGS[CURRENT_EO_CONFIG_NAME])\n",
        "\n",
        "\n",
        "# Initialize configuration\n",
        "config = Config()\n",
        "\n",
        "# Create output directories if they don't exist in Google Drive\n",
        "os.makedirs(config.OUTPUT_DIR, exist_ok=True)\n",
        "os.makedirs(config.CHECKPOINT_DIR, exist_ok=True)\n"
      ],
      "metadata": {
        "id": "Ga39ss3sphLn"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = Config()\n",
        "\n",
        "# Create output directories if they don't exist in Google Drive\n",
        "os.makedirs(config.OUTPUT_DIR, exist_ok=True)\n",
        "os.makedirs(config.CHECKPOINT_DIR, exist_ok=True)\n"
      ],
      "metadata": {
        "id": "Hx_jTwjxwKvs"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DataLoader and Preprocessing Custom Class\n"
      ],
      "metadata": {
        "id": "scEuwapdmkoy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Sen12MSDataset(Dataset):\n",
        "    def __init__(self, sar_dir, eo_dir, eo_bands, image_size=256):\n",
        "        self.sar_root = os.path.join(sar_dir)\n",
        "        self.eo_root = os.path.join(eo_dir)\n",
        "        self.eo_bands = eo_bands # List of band indices (1-indexed from original paper)\n",
        "        self.image_size = image_size\n",
        "\n",
        "        self.sar_image_paths = sorted(glob.glob(os.path.join(self.sar_root, '**', '*_s1_*.tif'), recursive=True))\n",
        "        # For EO, we need to find the base path for each image pair, then load specific bands\n",
        "        self.eo_base_paths = sorted(glob.glob(os.path.join(self.eo_root, '**', '*_s2_B*.tif'), recursive=True))\n",
        "\n",
        "        self.eo_image_groups = self._group_eo_files(self.eo_base_paths)\n",
        "\n",
        "        # to match SAR to EO\n",
        "        self.pairs = self._match_sar_eo_pairs()\n",
        "\n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.Resize(image_size, interpolation=transforms.InterpolationMode.BICUBIC),\n",
        "            transforms.ToTensor(), # Converts to [0, 1]\n",
        "        ])\n",
        "\n",
        "        print(f\"Found {len(self.pairs)} matched SAR-EO pairs.\")\n",
        "        if len(self.pairs) == 0:\n",
        "            print(\"WARNING: No SAR-EO pairs found. Please check your data paths and extraction.\")\n",
        "            print(f\"SAR root: {self.sar_root}\")\n",
        "            print(f\"EO root: {self.eo_root}\")\n",
        "            print(f\"Example SAR path search: {os.path.join(self.sar_root, '**', '*_s1_*.tif')}\")\n",
        "            print(f\"Example EO path search: {os.path.join(self.eo_root, '**', '*_s2_B*.tif')}\")\n",
        "\n",
        "\n",
        "    def _group_eo_files(self, eo_paths):\n",
        "        \"\"\"Groups EO band files by their common image ID.\"\"\"\n",
        "        groups = {}\n",
        "        for path in eo_paths:\n",
        "            base_name = '_'.join(os.path.basename(path).split('_')[:-1])\n",
        "            if base_name not in groups:\n",
        "                groups[base_name] = []\n",
        "            groups[base_name].append(path)\n",
        "        return groups\n",
        "\n",
        "    def _match_sar_eo_pairs(self):\n",
        "        \"\"\"\n",
        "        Matches SAR and EO image paths based on their common identifier.\n",
        "        Needs to be done beacause we need this while calculating PSNR, NDVI metrices\n",
        "        \"\"\"\n",
        "        matched_pairs = []\n",
        "\n",
        "        # Created a dictionary of SAR image IDs to their full paths\n",
        "        # SAR example: 'ROIs2017_winter_s1_21_p92.tif'\n",
        "        # We want to extract 'ROIs2017_winter_21_p92'\n",
        "        sar_ids_map = {}\n",
        "        for p in self.sar_image_paths:\n",
        "            base_name = os.path.basename(p)\n",
        "            parts = base_name.rsplit('_', 1) # Split from right once by '_'\n",
        "            if len(parts) > 1 and parts[-1].endswith('.tif'):\n",
        "                clean_id = parts[0].replace('_s1_', '_') # e.g., ROIs2017_winter_21_p92\n",
        "                sar_ids_map[clean_id] = p\n",
        "            else: # Fallback if naming is different\n",
        "                 clean_id = base_name.replace('.tif', '').replace('_s1_', '_')\n",
        "                 sar_ids_map[clean_id] = p\n",
        "\n",
        "\n",
        "        for eo_id_raw, eo_band_paths in self.eo_image_groups.items():\n",
        "            # eo_id_raw example: 'ROIs2017_winter_s2_21_p10_s2'\n",
        "            # We want to extract 'ROIs2017_winter_21_p10'\n",
        "            clean_eo_id = eo_id_raw.replace('_s2', '').replace('_s2_', '_') # e.g., ROIs2017_winter_21_p10\n",
        "\n",
        "            if clean_eo_id in sar_ids_map:\n",
        "                matched_pairs.append((sar_ids_map[clean_eo_id], eo_band_paths))\n",
        "            else:\n",
        "                print(f\"No matching SAR found for EO ID: {eo_id_raw} (cleaned: {clean_eo_id})\")\n",
        "\n",
        "        return matched_pairs\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.pairs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sar_path, eo_band_paths = self.pairs[idx]\n",
        "\n",
        "        sar_image_pil = Image.open(sar_path) # loads image\n",
        "\n",
        "        # Convert to numpy array to handle channels if PIL doesn't load it as multi-channel directly\n",
        "        sar_image_np = np.array(sar_image_pil)\n",
        "\n",
        "        # If SAR is grayscale (H, W), convert to (H, W, 1) then to (1, H, W)\n",
        "        if sar_image_np.ndim == 2:\n",
        "            sar_image_np = sar_image_np[:, :, np.newaxis] # Add channel dim (H, W, 1)\n",
        "\n",
        "\n",
        "        if sar_image_np.dtype == np.uint16:\n",
        "            sar_image_tensor = torch.from_numpy(sar_image_np.astype(np.float32)) / 65535.0 # Scale to [0, 1]\n",
        "        else:\n",
        "            sar_image_tensor = torch.from_numpy(sar_image_np).float()\n",
        "\n",
        "        sar_image_tensor = torch.nan_to_num(sar_image_tensor, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "\n",
        "        if sar_image_tensor.ndim == 3 and sar_image_tensor.shape[0] not in [1, 2]: # If first dim is not channel count\n",
        "             sar_image_tensor = sar_image_tensor.permute(2, 0, 1) # Assuming (H, W, C) to (C, H, W)\n",
        "\n",
        "        # Normalize SAR to [-1, 1] (after initial [0,1] scaling from uint16 or float conversion)\n",
        "        # Re-normalize to [-1, 1] based on the current tensor's min/max for robustness\n",
        "        sar_min = sar_image_tensor.min()\n",
        "        sar_max = sar_image_tensor.max()\n",
        "        if sar_max > sar_min:\n",
        "            sar_image_tensor = (sar_image_tensor - sar_min) / (sar_max - sar_min) # Scale to [0, 1]\n",
        "        else:\n",
        "            sar_image_tensor = torch.zeros_like(sar_image_tensor)\n",
        "        sar_image_tensor = sar_image_tensor * 2.0 - 1.0 # Scale to [-1, 1]\n",
        "\n",
        "        # Resize SAR image\n",
        "        sar_image_tensor = transforms.Resize(self.image_size, interpolation=transforms.InterpolationMode.BICUBIC)(sar_image_tensor)\n",
        "\n",
        "\n",
        "        eo_images_list = []\n",
        "        sorted_eo_band_paths = sorted(eo_band_paths, key=lambda x: int(os.path.basename(x).split('_B')[-1].split('.')[0]))\n",
        "\n",
        "        for band_idx in self.eo_bands:\n",
        "            band_path = next((p for p in sorted_eo_band_paths if f'_B{band_idx}.tif' in p), None)\n",
        "            if band_path is None:\n",
        "                print(f\"Warning: Band B{band_idx} not found for EO image group {os.path.basename(os.path.dirname(sar_path))}. Filling with zeros.\")\n",
        "                dummy_array = np.zeros((self.image_size, self.image_size), dtype=np.uint16)\n",
        "                eo_band_img = Image.fromarray(dummy_array)\n",
        "            else:\n",
        "                eo_band_img = Image.open(band_path).convert('I') # 'I' for 32-bit signed integer pixels\n",
        "\n",
        "            eo_images_list.append(self.transform(eo_band_img))\n",
        "\n",
        "        eo_image_tensor = torch.cat(eo_images_list, dim=0)\n",
        "\n",
        "        eo_max_val = 10000.0\n",
        "        eo_image_tensor = torch.clamp(eo_image_tensor, 0, eo_max_val)\n",
        "        eo_image_tensor = (eo_image_tensor / eo_max_val) * 2.0 - 1.0\n",
        "\n",
        "        return sar_image_tensor, eo_image_tensor\n"
      ],
      "metadata": {
        "id": "9dlIpfH9ILjW"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Helper function for Convolutional Block\n",
        "def conv_block(in_channels, out_channels, kernel_size, stride, padding, use_bias=False, norm_layer=nn.InstanceNorm2d, activation=nn.ReLU(True)):\n",
        "    \"\"\"A convolutional block with Conv2d, Normalization, and Activation.\"\"\"\n",
        "    layers = [\n",
        "        nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, bias=use_bias)\n",
        "    ]\n",
        "    if norm_layer:\n",
        "        layers.append(norm_layer(out_channels))\n",
        "    if activation:\n",
        "        layers.append(activation)\n",
        "    return nn.Sequential(*layers)\n",
        "\n",
        "# Helper function for Transposed Convolutional Block (for upsampling)\n",
        "def deconv_block(in_channels, out_channels, kernel_size, stride, padding, output_padding, use_bias=False, norm_layer=nn.InstanceNorm2d, activation=nn.ReLU(True)):\n",
        "    \"\"\"A transposed convolutional block with ConvTranspose2d, Normalization, and Activation.\"\"\"\n",
        "    layers = [\n",
        "        nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride, padding, output_padding, bias=use_bias)\n",
        "    ]\n",
        "    if norm_layer:\n",
        "        layers.append(norm_layer(out_channels))\n",
        "    if activation:\n",
        "        layers.append(activation)\n",
        "    return nn.Sequential(*layers)\n"
      ],
      "metadata": {
        "id": "ixHuQGmpMSqe"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kQgQyMakAnHY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}