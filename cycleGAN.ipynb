{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Disha-Sikka/SAR-to-EO-CycleGAN/blob/main/cycleGAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5O0WzZ17LDUv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "563735f7-093e-4c52-91b7-93d34e32a561"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "acjJOSnyex6Q"
      },
      "source": [
        "# Copying Extracted files in Colab's Local Disk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "q1CywYH9ahp-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7407e3f3-1299-46e0-94d1-d9ef3a2dcf63"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Copying ROIs2017_winter_s1.tar.gz from Drive to Colab local disk...\n",
            "S1 file copied.\n",
            "Copying ROIs2017_winter_s2.tar.gz from Drive to Colab local disk...\n",
            "S2 file copied.\n",
            "Copying complete.\n"
          ]
        }
      ],
      "source": [
        "import shutil\n",
        "import os\n",
        "\n",
        "# store location of files in drive. So, that we can copy them\n",
        "drive_path_s1 = '/content/drive/MyDrive/CycleGAN/winter_s1'\n",
        "drive_path_s2 = '/content/drive/MyDrive/CycleGAN/winter_s2'\n",
        "\n",
        "Data_Root_Dir= '/content/my_sen12ms_data_subset'\n",
        "os.makedirs(Data_Root_Dir, exist_ok=True)\n",
        "\n",
        "# store location of colab's paths. Where you want to copy files\n",
        "colab_path_s1 = os.path.join(Data_Root_Dir, 'winter_s1')\n",
        "colab_path_s2 = os.path.join(Data_Root_Dir, 'winter_s2')\n",
        "\n",
        "print(\"Copying ROIs2017_winter_s1.tar.gz from Drive to Colab local disk...\")\n",
        "if os.path.exists(drive_path_s1):\n",
        "    shutil.copytree(drive_path_s1, colab_path_s1) # copytree --> is used to copy a folder while copy is used to copy a zip file\n",
        "    print(\"S1 file copied.\")\n",
        "else:\n",
        "    print(\"Wrong Path\")\n",
        "\n",
        "print(\"Copying ROIs2017_winter_s2.tar.gz from Drive to Colab local disk...\")\n",
        "if os.path.exists(drive_path_s2):\n",
        "    shutil.copytree(drive_path_s2, colab_path_s2)\n",
        "    print(\"S2 file copied.\")\n",
        "else:\n",
        "    print(\"Wrong Path\")\n",
        "\n",
        "print(\"Copying complete.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lt6EuG5EpP59"
      },
      "source": [
        "# Importing Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "R0gKf_UlpM8i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1cf108c1-ae81-4fdf-ea73-e5aeb7fb8bdc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting rasterio\n",
            "  Downloading rasterio-1.4.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.1 kB)\n",
            "Collecting affine (from rasterio)\n",
            "  Downloading affine-2.4.0-py3-none-any.whl.metadata (4.0 kB)\n",
            "Requirement already satisfied: attrs in /usr/local/lib/python3.11/dist-packages (from rasterio) (25.3.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from rasterio) (2025.7.14)\n",
            "Requirement already satisfied: click>=4.0 in /usr/local/lib/python3.11/dist-packages (from rasterio) (8.2.1)\n",
            "Collecting cligj>=0.5 (from rasterio)\n",
            "  Downloading cligj-0.7.2-py3-none-any.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: numpy>=1.24 in /usr/local/lib/python3.11/dist-packages (from rasterio) (2.0.2)\n",
            "Collecting click-plugins (from rasterio)\n",
            "  Downloading click_plugins-1.1.1.2-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from rasterio) (3.2.3)\n",
            "Downloading rasterio-1.4.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (22.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22.2/22.2 MB\u001b[0m \u001b[31m84.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cligj-0.7.2-py3-none-any.whl (7.1 kB)\n",
            "Downloading affine-2.4.0-py3-none-any.whl (15 kB)\n",
            "Downloading click_plugins-1.1.1.2-py2.py3-none-any.whl (11 kB)\n",
            "Installing collected packages: cligj, click-plugins, affine, rasterio\n",
            "Successfully installed affine-2.4.0 click-plugins-1.1.1.2 cligj-0.7.2 rasterio-1.4.3\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import glob\n",
        "import random\n",
        "import numpy as np\n",
        "from PIL import Image, UnidentifiedImageError\n",
        "from tqdm import tqdm\n",
        "!pip install rasterio\n",
        "import rasterio\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.utils as vutils\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from skimage.metrics import structural_similarity as ssim\n",
        "from skimage.metrics import peak_signal_noise_ratio as psnr"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vxeNC3Lqucof"
      },
      "source": [
        "# Configuration\n",
        "\n",
        "- For defining the parameters. So, that we can easily change them when we want."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "Ga39ss3sphLn"
      },
      "outputs": [],
      "source": [
        "class Config:\n",
        "    # Data paths\n",
        "    data_root_dir='/content/my_sen12ms_data_subset'\n",
        "    SAR_DIR = 'winter_s1'\n",
        "    EO_DIR = 'winter_s2'\n",
        "\n",
        "    # Model parameters\n",
        "    INPUT_NC = 3 # Number of input channels for SAR (Sentinel-1 GRD usually has 2: VV, VH)\n",
        "    NGF = 64 # Number of generator filters in the first conv layer\n",
        "    NDF = 64 # Number of discriminator filters in the first conv layer\n",
        "    N_RESNET_BLOCKS = 6 # Number of ResNet blocks in the generator\n",
        "\n",
        "    # Training parameters\n",
        "    BATCH_SIZE = 1 # CycleGAN typically uses batch size 1\n",
        "    NUM_EPOCHS = 5\n",
        "    LR = 0.0002 # Learning rate\n",
        "    BETA1 = 0.5 # Adam optimizer beta1\n",
        "    LAMBDA_CYCLE = 10.0 # Weight for cycle consistency loss\n",
        "    LAMBDA_IDENTITY = 5.0 # Weight for identity mapping loss helps stabilize\n",
        "\n",
        "    # Image parameters\n",
        "    IMAGE_SIZE = 256\n",
        "    NUM_WORKERS = 4\n",
        "\n",
        "    # Output and logging\n",
        "    # Save outputs and checkpoints to Google Drive for persistence across sessions\n",
        "    OUTPUT_BASE_DIR = '/content/drive/MyDrive/CycleGAN/SAR_EO_Project_Outputs' # Base directory in Drive\n",
        "    OUTPUT_DIR = os.path.join(OUTPUT_BASE_DIR, 'output_cyclegan') # Specific output for images\n",
        "    CHECKPOINT_DIR = os.path.join(OUTPUT_BASE_DIR, 'checkpoints_cyclegan') # Specific output for models\n",
        "\n",
        "    SAVE_EPOCH_FREQ = 5 # Save model checkpoints every N epochs\n",
        "    PRINT_FREQ = 1 # Print training loss every N batches\n",
        "\n",
        "    # Device\n",
        "    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    # EO Band Configurations (Sentinel-2 bands)\n",
        "    # B1 (Coastal Aerosol), B2 (Blue), B3 (Green), B4 (Red), B5 (Red Edge 1),\n",
        "    # B6 (Red Edge 2), B7 (Red Edge 3), B8 (NIR), B8A (NIR Narrow), B9 (Water Vapour),\n",
        "    # B10 (SWIR - Cirrus), B11 (SWIR 1), B12 (SWIR 2)\n",
        "    EO_BAND_CONFIGS = {\n",
        "        \"RGB\": [4, 3, 2], # B4, B3, B2 (Red, Green, Blue)\n",
        "        \"NIR_SWIR_RedEdge\": [8, 11, 5], # B8, B11, B5 (NIR, SWIR1, Red Edge 1)\n",
        "        \"RGB_NIR\": [4, 3, 2, 8] # B4, B3, B2, B8 (Red, Green, Blue, NIR)\n",
        "    }\n",
        "\n",
        "    CURRENT_EO_CONFIG_NAME = \"RGB\"\n",
        "    OUTPUT_NC = len(EO_BAND_CONFIGS[CURRENT_EO_CONFIG_NAME])\n",
        "\n",
        "\n",
        "# Initialize configuration\n",
        "config = Config()\n",
        "\n",
        "# Create output directories if they don't exist in Google Drive\n",
        "os.makedirs(config.OUTPUT_DIR, exist_ok=True)\n",
        "os.makedirs(config.CHECKPOINT_DIR, exist_ok=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Hx_jTwjxwKvs"
      },
      "outputs": [],
      "source": [
        "config = Config()\n",
        "\n",
        "# Create output directories if they don't exist in Google Drive\n",
        "os.makedirs(config.OUTPUT_DIR, exist_ok=True)\n",
        "os.makedirs(config.CHECKPOINT_DIR, exist_ok=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "scEuwapdmkoy"
      },
      "source": [
        "# DataLoader and Preprocessing Custom Class\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "9dlIpfH9ILjW"
      },
      "outputs": [],
      "source": [
        "class Sen12MSDataset(Dataset):\n",
        "  def __init__(self, root_dir, sar_dir, eo_dir, eo_bands, image_size=256):\n",
        "    self.sar_root = os.path.join(root_dir, sar_dir)\n",
        "    self.eo_root = os.path.join(root_dir, eo_dir)\n",
        "    self.eo_bands = eo_bands # List of band numbers (e.g., [4, 3, 2]) from Config\n",
        "    self.image_size = image_size\n",
        "\n",
        "    # Find all .tif files within the SAR root, recursively\n",
        "    self.sar_image_paths = sorted(glob.glob(os.path.join(self.sar_root, '**', '*.tif'), recursive=True))\n",
        "\n",
        "    # Find all .tif files within the EO root, recursively\n",
        "    self.eo_image_paths = sorted(glob.glob(os.path.join(self.eo_root, '**', '*.tif'), recursive=True))\n",
        "\n",
        "    # Filter out unreadable/problematic files during initialization\n",
        "    self.sar_image_paths = self._filter_unreadable_images(self.sar_image_paths, \"SAR\")\n",
        "    self.eo_image_paths = self._filter_unreadable_images(self.eo_image_paths, \"EO\")\n",
        "\n",
        "\n",
        "    self.pairs = self._match_sar_eo_pairs()\n",
        "\n",
        "\n",
        "    print(f\"Found {len(self.pairs)} matched SAR-EO pairs.\")\n",
        "    if len(self.pairs) == 0:\n",
        "        print(\"WARNING: No SAR-EO pairs found. Please check your data paths and extraction.\")\n",
        "        print(f\"SAR root: {self.sar_root}\")\n",
        "        print(f\"EO root: {self.eo_root}\")\n",
        "        print(f\"Example SAR path search: {os.path.join(self.sar_root, '**', '*.tif')}\")\n",
        "        print(f\"Example EO path search: {os.path.join(self.eo_root, '**', '*.tif')}\")\n",
        "\n",
        "\n",
        "  def _filter_unreadable_images(self, image_paths, image_type=\"Image\"):\n",
        "        \"\"\"\n",
        "        Checks if images are readable by rasterio and filters out unreadable ones.\n",
        "        \"\"\"\n",
        "        readable_paths = []\n",
        "        for path in tqdm(image_paths, desc=f\"Checking {image_type} readability\"):\n",
        "            try:\n",
        "                with rasterio.open(path) as src:\n",
        "                    # Try to read a band to ensure it's truly readable\n",
        "                    _ = src.read(1)\n",
        "                readable_paths.append(path)\n",
        "            except Exception as e: # Catch broader exceptions for rasterio issues\n",
        "                print(f\"Skipping unreadable {image_type} file: {path} (Error: {e})\")\n",
        "        return readable_paths\n",
        "\n",
        "\n",
        "  def _match_sar_eo_pairs(self):\n",
        "      \"\"\"\n",
        "      Creates pseudo-pairs for unpaired training. Each SAR image is paired with a random EO image.\n",
        "      This ensures the dataset has a non-zero length and provides data for training.\n",
        "      \"\"\"\n",
        "      if not self.sar_image_paths or not self.eo_image_paths:\n",
        "          return [] # No data found\n",
        "\n",
        "      matched_pairs = []\n",
        "      for sar_path in self.sar_image_paths:\n",
        "          random_eo_path = random.choice(self.eo_image_paths)\n",
        "          matched_pairs.append((sar_path, random_eo_path))\n",
        "\n",
        "      return matched_pairs\n",
        "\n",
        "\n",
        "  def __len__(self):\n",
        "      return len(self.pairs)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "      sar_path, eo_path = self.pairs[idx]\n",
        "\n",
        "      # --- Load and Preprocess SAR Image ---\n",
        "     # --- Load and Preprocess SAR Image ---\n",
        "      with rasterio.open(sar_path) as src:\n",
        "          sar_image_np_raw = src.read().astype(np.float32) # Read all bands, convert to float32\n",
        "\n",
        "      # --- NEW ROBUST SAR CHANNEL HANDLING ---\n",
        "      # Ensure sar_image_np has exactly config.INPUT_NC channels (which is 2)\n",
        "      if sar_image_np_raw.shape[0] == config.INPUT_NC:\n",
        "          sar_image_np = sar_image_np_raw # Perfect match\n",
        "      elif sar_image_np_raw.shape[0] == 1 and config.INPUT_NC == 2:\n",
        "      #     # If 1-channel, but 2 expected, repeat it (e.g., VV -> VV,VV)\n",
        "          sar_image_np = np.repeat(sar_image_np_raw, 2, axis=0)\n",
        "          print(f\"Warning: SAR image {os.path.basename(sar_path)} is 1-channel ({sar_image_np_raw.shape[0]} channels), but INPUT_NC is {config.INPUT_NC}. Repeating channel.\")\n",
        "      elif sar_image_np_raw.shape[0] == 3 and config.INPUT_NC == 2:\n",
        "      #     # If 3-channel (e.g., RGB), but 2 expected, take first 2 channels\n",
        "          sar_image_np = sar_image_np_raw[:2, :, :]\n",
        "          print(f\"Warning: SAR image {os.path.basename(sar_path)} is 3-channel, but INPUT_NC is {config.INPUT_NC}. Taking first 2 channels.\")\n",
        "      elif sar_image_np_raw.shape[0] > config.INPUT_NC:\n",
        "      #     # If more channels than expected (and not 3-channel handled above), take the first INPUT_NC channels\n",
        "          sar_image_np = sar_image_np_raw[:config.INPUT_NC, :, :]\n",
        "          print(f\"Warning: SAR image {os.path.basename(sar_path)} has {sar_image_np_raw.shape[0]} channels, but INPUT_NC is {config.INPUT_NC}. Taking first {config.INPUT_NC} channels.\")\n",
        "      else: # sar_image_np_raw.shape[0] < config.INPUT_NC\n",
        "      #     # Pad with zeros if fewer channels than expected\n",
        "          padding_needed = config.INPUT_NC - sar_image_np_raw.shape[0]\n",
        "          sar_image_np = np.pad(sar_image_np_raw, ((0, padding_needed), (0,0), (0,0)), mode='constant')\n",
        "          print(f\"Warning: SAR image {os.path.basename(sar_path)} has {sar_image_np_raw.shape[0]} channels, but INPUT_NC is {config.INPUT_NC}. Padding with zeros.\")\n",
        "\n",
        "\n",
        "      sar_image_tensor = torch.from_numpy(sar_image_np).float()\n",
        "      sar_image_tensor = torch.nan_to_num(sar_image_tensor, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "\n",
        "      sar_image_tensor = F.interpolate(sar_image_tensor.unsqueeze(0), size=(self.image_size, self.image_size), mode='bicubic', align_corners=False).squeeze(0)\n",
        "\n",
        "      sar_min = sar_image_tensor.min()\n",
        "      sar_max = sar_image_tensor.max()\n",
        "      if sar_max > sar_min:\n",
        "            sar_image_tensor = (sar_image_tensor - sar_min) / (sar_max - sar_min) # Scale to [0, 1]\n",
        "      else:\n",
        "            sar_image_tensor = torch.zeros_like(sar_image_tensor)\n",
        "      sar_image_tensor = sar_image_tensor * 2.0 - 1.0 # Scale to [-1, 1]\n",
        "\n",
        "      # Load and Preprocess EO Image (3-channel RGB .tif)\n",
        "      with rasterio.open(eo_path) as src:\n",
        "            eo_image_np = src.read().astype(np.float32) # Read all bands, convert to float32\n",
        "            # src.read() reads (C, H, W) directly.\n",
        "\n",
        "      eo_image_tensor_full = torch.from_numpy(eo_image_np).float()\n",
        "\n",
        "      # find this index and band number using QGIS\n",
        "      band_number_to_channel_index = {\n",
        "          4: 0, # Sentinel-2 Band 4 (Red) is at channel index 0\n",
        "          3: 1, # Sentinel-2 Band 3 (Green) is at channel index 1\n",
        "          2: 2, # Sentinel-2 Band 2 (Blue) is at channel index 2\n",
        "      }\n",
        "\n",
        "      selected_eo_bands_tensors = []\n",
        "      for band_num in self.eo_bands: # Iterate through desired band numbers from Config (e.g., 4, 3, 2)\n",
        "          channel_idx = band_number_to_channel_index.get(band_num, -1)\n",
        "\n",
        "          if channel_idx == -1 or channel_idx >= eo_image_tensor_full.shape[0]:\n",
        "              print(f\"Warning: Desired band B{band_num} not found or index out of range in EO image {os.path.basename(eo_path)}. Filling with zeros.\")\n",
        "              # Create a zero-filled channel if the band is not present in the loaded TIFF\n",
        "              selected_eo_bands_tensors.append(torch.zeros(1, eo_image_tensor_full.shape[1], eo_image_tensor_full.shape[2], device=config.DEVICE))\n",
        "          else:\n",
        "              selected_eo_bands_tensors.append(eo_image_tensor_full[channel_idx:channel_idx+1, :, :])\n",
        "\n",
        "      if not selected_eo_bands_tensors:\n",
        "          print(f\"Error: No valid bands selected for EO image {os.path.basename(eo_path)}. Returning zeros.\")\n",
        "          eo_image_tensor = torch.zeros(config.OUTPUT_NC, self.image_size, self.image_size, device=config.DEVICE)\n",
        "      else:\n",
        "          eo_image_tensor = torch.cat(selected_eo_bands_tensors, dim=0)\n",
        "\n",
        "      # Ensure the final EO tensor has the correct number of channels (OUTPUT_NC)\n",
        "      if eo_image_tensor.shape[0] != config.OUTPUT_NC:\n",
        "          print(f\"Error: Final EO tensor has {eo_image_tensor.shape[0]} channels, but expected {config.OUTPUT_NC}. This indicates an issue with band selection or config.OUTPUT_NC.\")\n",
        "          if eo_image_tensor.shape[0] < config.OUTPUT_NC:\n",
        "              padding_needed = config.OUTPUT_NC - eo_image_tensor.shape[0]\n",
        "              eo_image_tensor = torch.cat([eo_image_tensor, torch.zeros(padding_needed, eo_image_tensor.shape[1], eo_image_tensor.shape[2], device=config.DEVICE)], dim=0)\n",
        "          else:\n",
        "              eo_image_tensor = eo_image_tensor[:config.OUTPUT_NC, :, :]\n",
        "\n",
        "\n",
        "      # Normalize EO to [-1, 1]\n",
        "      eo_max_val = 10000.0\n",
        "      eo_image_tensor = torch.clamp(eo_image_tensor, 0, eo_max_val)\n",
        "      eo_image_tensor = (eo_image_tensor / eo_max_val) * 2.0 - 1.0\n",
        "\n",
        "      eo_image_tensor = F.interpolate(eo_image_tensor.unsqueeze(0), size=(self.image_size, self.image_size), mode='bicubic', align_corners=False).squeeze(0)\n",
        "\n",
        "      return sar_image_tensor, eo_image_tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ixHuQGmpMSqe"
      },
      "outputs": [],
      "source": [
        "# Helper function for Convolutional Block\n",
        "def conv_block(in_channels, out_channels, kernel_size, stride, padding, use_bias=False, norm_layer=nn.InstanceNorm2d, activation=nn.ReLU(True)):\n",
        "    \"\"\"A convolutional block with Conv2d, Normalization, and Activation.\"\"\"\n",
        "    layers = [\n",
        "        nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, bias=use_bias)\n",
        "    ]\n",
        "    if norm_layer:\n",
        "        layers.append(norm_layer(out_channels))\n",
        "    if activation:\n",
        "        layers.append(activation)\n",
        "    return nn.Sequential(*layers)\n",
        "\n",
        "# Helper function for Transposed Convolutional Block (for upsampling)\n",
        "def deconv_block(in_channels, out_channels, kernel_size, stride, padding, output_padding, use_bias=False, norm_layer=nn.InstanceNorm2d, activation=nn.ReLU(True)):\n",
        "    \"\"\"A transposed convolutional block with ConvTranspose2d, Normalization, and Activation.\"\"\"\n",
        "    layers = [\n",
        "        nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride, padding, output_padding, bias=use_bias)\n",
        "    ]\n",
        "    if norm_layer:\n",
        "        layers.append(norm_layer(out_channels))\n",
        "    if activation:\n",
        "        layers.append(activation)\n",
        "    return nn.Sequential(*layers)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZAOBjmLjDiRL"
      },
      "source": [
        "# Class ResNet\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "kQgQyMakAnHY"
      },
      "outputs": [],
      "source": [
        "class ResnetBlock(nn.Module):\n",
        "    def __init__(self, dim, norm_layer=nn.InstanceNorm2d, use_bias=False):\n",
        "        super(ResnetBlock, self).__init__()\n",
        "        self.conv_block = nn.Sequential(\n",
        "            nn.ReflectionPad2d(1),\n",
        "            conv_block(dim, dim, kernel_size=3, stride=1, padding=0, use_bias=use_bias, norm_layer=norm_layer),\n",
        "            nn.Dropout(0.5), # Added dropout for regularization\n",
        "            nn.ReflectionPad2d(1),\n",
        "            conv_block(dim, dim, kernel_size=3, stride=1, padding=0, use_bias=use_bias, norm_layer=norm_layer, activation=None) # No activation after second conv\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.conv_block(x) # Residual connection\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H4z_qi-BHRcT"
      },
      "source": [
        "# Class Generator\n",
        "- U-Net Based\n",
        "- Translates images from Domain A to Domain B\n",
        "- In this first we downsample the image, than pass the image through ResNet Block, than we decode the image by upscaling it and adding a conv layer which converts it to RGB format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "dCXifUd9HP5n"
      },
      "outputs": [],
      "source": [
        "class Generator(nn.Module):\n",
        "    \"\"\"\n",
        "    Generator model (U-Net based).\n",
        "    Translates images from domain A to domain B.\n",
        "    \"\"\"\n",
        "    def __init__(self, input_nc, output_nc, ngf=64, n_blocks=9, norm_layer=nn.InstanceNorm2d):\n",
        "        super(Generator, self).__init__()\n",
        "        use_bias = norm_layer == nn.InstanceNorm2d\n",
        "\n",
        "        model = [\n",
        "            nn.ReflectionPad2d(3),\n",
        "            conv_block(input_nc, ngf, kernel_size=7, stride=1, padding=0, use_bias=use_bias, norm_layer=norm_layer)\n",
        "        ]\n",
        "\n",
        "        # Downsampling\n",
        "        n_downsampling = 2\n",
        "        for i in range(n_downsampling):\n",
        "            mult = 2 ** i\n",
        "            model += [\n",
        "                conv_block(ngf * mult, ngf * mult * 2, kernel_size=3, stride=2, padding=1, use_bias=use_bias, norm_layer=norm_layer)\n",
        "            ]\n",
        "\n",
        "        # ResNet blocks\n",
        "        mult = 2 ** n_downsampling\n",
        "        for i in range(n_blocks):\n",
        "            model += [ResnetBlock(ngf * mult, norm_layer=norm_layer, use_bias=use_bias)]\n",
        "\n",
        "        # Upsampling\n",
        "        for i in range(n_downsampling):\n",
        "            mult = 2 ** (n_downsampling - i)\n",
        "            model += [\n",
        "                deconv_block(ngf * mult, int(ngf * mult / 2), kernel_size=3, stride=2, padding=1, output_padding=1, use_bias=use_bias, norm_layer=norm_layer)\n",
        "            ]\n",
        "\n",
        "        model += [\n",
        "            nn.ReflectionPad2d(3),\n",
        "            nn.Conv2d(ngf, output_nc, kernel_size=7, padding=0),\n",
        "            nn.Tanh() # Output activation to map to [-1, 1]\n",
        "        ]\n",
        "        self.model = nn.Sequential(*model)\n",
        "\n",
        "    def forward(self, input):\n",
        "        return self.model(input)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pq6-LqHnLaVS"
      },
      "source": [
        "# Discriminator Class\n",
        "\n",
        "- PatchGAN based\n",
        "- Classifies whether the provided image is real or fake\n",
        "- Convulational layer and Normalization while downscaling.\n",
        "- At last layer, Conv layer and Sigmoid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "phRrfhwUIXAj"
      },
      "outputs": [],
      "source": [
        "\n",
        "class Discriminator(nn.Module):\n",
        "    \"\"\"\n",
        "    Discriminator model (PatchGAN based).\n",
        "    Classifies image patches as real or fake.\n",
        "    \"\"\"\n",
        "    def __init__(self, input_nc, ndf=64, n_layers=3, norm_layer=nn.InstanceNorm2d):\n",
        "        super(Discriminator, self).__init__()\n",
        "        use_bias = norm_layer == nn.InstanceNorm2d\n",
        "\n",
        "        kw = 4 # Kernel width/height\n",
        "        padw = 1 # Padding\n",
        "\n",
        "        model = [\n",
        "            nn.Conv2d(input_nc, ndf, kernel_size=kw, stride=2, padding=padw),\n",
        "            nn.LeakyReLU(0.2, True) # Leaky ReLU prevents dying ReLU problems and allow gradients to flow for negative inputs\n",
        "        ]\n",
        "\n",
        "        for i in range(1, n_layers):\n",
        "            mult = 2 ** i\n",
        "            model += [\n",
        "                conv_block(ndf * mult // 2, ndf * mult, kernel_size=kw, stride=2, padding=padw, use_bias=use_bias, norm_layer=norm_layer, activation=nn.LeakyReLU(0.2, True))\n",
        "            ]\n",
        "\n",
        "        mult = 2 ** n_layers\n",
        "        model += [\n",
        "            conv_block(ndf * mult // 2, ndf * mult, kernel_size=kw, stride=1, padding=padw, use_bias=use_bias, norm_layer=norm_layer, activation=nn.LeakyReLU(0.2, True))\n",
        "        ]\n",
        "\n",
        "        model += [\n",
        "            nn.Conv2d(ndf * mult, 1, kernel_size=kw, stride=1, padding=padw) # Output 1-channel prediction map\n",
        "        ]\n",
        "        self.model = nn.Sequential(*model)\n",
        "\n",
        "    def forward(self, input):\n",
        "        return self.model(input)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vhZFraNuUcW1"
      },
      "source": [
        "# Adversarial Loss Function\n",
        "\n",
        "- It tells how well discriminator is working in distinguishing real and fake.\n",
        "- It also tells how well generator is working in translating the image from one domain to other domain.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "CxqMQN9DM9YJ"
      },
      "outputs": [],
      "source": [
        "class GANLoss(nn.Module):\n",
        "    \"\"\"\n",
        "    Adversarial Loss calculation of discriminator and generator functions. Loss Calculated using mean square\n",
        "    \"\"\"\n",
        "    def __init__(self, gan_mode='mse', target_real_label=1.0, target_fake_label=0.0):\n",
        "        super(GANLoss, self).__init__()\n",
        "        self.register_buffer('real_label', torch.tensor(target_real_label))\n",
        "        self.register_buffer('fake_label', torch.tensor(target_fake_label))\n",
        "        self.gan_mode = gan_mode\n",
        "        if gan_mode == 'lsgan': # Least Squares GAN\n",
        "            self.loss = nn.MSELoss()\n",
        "        elif gan_mode == 'vanilla': # Standard GAN (Binary Cross Entropy)\n",
        "            self.loss = nn.BCEWithLogitsLoss()\n",
        "        else:\n",
        "            raise NotImplementedError(f'GAN mode {gan_mode} not implemented.')\n",
        "\n",
        "    def get_target_tensor(self, prediction, target_is_real):\n",
        "        \"\"\"Creates label tensors with the same size as the input prediction.\"\"\"\n",
        "        if target_is_real:\n",
        "            target_tensor = self.real_label\n",
        "        else:\n",
        "            target_tensor = self.fake_label\n",
        "        return target_tensor.expand_as(prediction)\n",
        "\n",
        "    def __call__(self, prediction, target_is_real):\n",
        "        target_tensor = self.get_target_tensor(prediction, target_is_real)\n",
        "        return self.loss(prediction, target_tensor)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qr-TotPiXq_t"
      },
      "source": [
        "# Cycle Consistency Loss\n",
        "\n",
        "- It calculates the loss while translating SAR to EO and than back to SAR\n",
        "- Mean Absolute Error"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "DUzxHls7W7m1"
      },
      "outputs": [],
      "source": [
        "class CycleConsistencyLoss(nn.Module):\n",
        "    \"\"\"\n",
        "    L1 loss for cycle consistency.\n",
        "    \"\"\"\n",
        "    def __init__(self, lambda_cycle=10.0): # higher the lambda--> higher is the consistency of cycle\n",
        "        super(CycleConsistencyLoss, self).__init__()\n",
        "        self.lambda_cycle = lambda_cycle\n",
        "        self.loss = nn.L1Loss() # because it encourages pixel-wise accuracy\n",
        "\n",
        "    def forward(self, real_image, cycled_image):\n",
        "        return self.loss(real_image, cycled_image) * self.lambda_cycle\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zWr4O3VwdZ8Z"
      },
      "source": [
        "# Identity Loss\n",
        "\n",
        "- Optional\n",
        "- Done for colour preservation\n",
        "- And training Stability\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "SJpSaV8GalYo"
      },
      "outputs": [],
      "source": [
        "class IdentityLoss(nn.Module):\n",
        "    \"\"\"\n",
        "    L1 loss for identity mapping.\n",
        "    Encourages generators to preserve color composition when input is already from the target domain.\n",
        "    \"\"\"\n",
        "    def __init__(self, lambda_identity=5.0):\n",
        "        super(IdentityLoss, self).__init__()\n",
        "        self.lambda_identity = lambda_identity\n",
        "        self.loss = nn.L1Loss()\n",
        "\n",
        "    def forward(self, real_image, identity_image):\n",
        "        return self.loss(real_image, identity_image) * self.lambda_identity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gZAMJJrzmV0q"
      },
      "source": [
        "# Utilities for Postprocessing and Metrics\n",
        "\n",
        "- To convert pixels value from the range [-1,1] to [0,1] or [0,255]\n",
        "- For showing the images SAR and generated EO side by side.\n",
        "- To measure the similarity between two images.\n",
        "- To calculate peak-signal-to-noise (PSNR) ratio.\n",
        "- To calculate NDVI from EO image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "t4EtYp_mmXF2"
      },
      "outputs": [],
      "source": [
        "def denormalize_image(tensor):\n",
        "    \"\"\"\n",
        "    Denormalizes a tensor from [-1, 1] to [0, 1].\n",
        "    Args:\n",
        "        tensor (torch.Tensor): Image tensor in range [-1, 1].\n",
        "    Returns:\n",
        "        torch.Tensor: Image tensor in range [0, 1].\n",
        "    \"\"\"\n",
        "    return (tensor + 1) / 2.0\n",
        "\n",
        "def save_combined_image(sar_img, gen_eo_img, real_eo_img, filename):\n",
        "    \"\"\"\n",
        "    Combines Real SAR, Generated EO, and Real EO images side-by-side and saves them.\n",
        "    Assumes inputs are already denormalized to [0, 1].\n",
        "    \"\"\"\n",
        "    if sar_img.shape[1] == 2: # Check channel dimension (N, C, H, W)\n",
        "        sar_img_display = sar_img.mean(dim=1, keepdim=True).repeat(1, 3, 1, 1) # Convert to grayscale 3-channel\n",
        "    else:\n",
        "        sar_img_display = sar_img.repeat(1, 3, 1, 1) if sar_img.shape[1] == 1 else sar_img\n",
        "\n",
        "    # For EO, if it has 4 channels (RGB+NIR), select RGB for display\n",
        "    if gen_eo_img.shape[1] == 4:\n",
        "        gen_eo_img_display = gen_eo_img[:, :3, :, :] # Take first 3 channels (RGB)\n",
        "        real_eo_img_display = real_eo_img[:, :3, :, :]\n",
        "    else:\n",
        "        gen_eo_img_display = gen_eo_img\n",
        "        real_eo_img_display = real_eo_img\n",
        "\n",
        "    # Concatenate images horizontally\n",
        "    combined_image = torch.cat([sar_img_display, gen_eo_img_display, real_eo_img_display], dim=3) # Concatenate along width\n",
        "    vutils.save_image(combined_image, filename, normalize=True, nrow=1) # normalize=True scales each image in the grid to [0,1]ng)\n",
        "\n",
        "def calculate_ssim(img1, img2, data_range=1.0, multichannel=True):\n",
        "    \"\"\"\n",
        "    Calculates SSIM between two images.\n",
        "    Args:\n",
        "        img1 (torch.Tensor): First image (N, C, H, W) or (C, H, W) in range [0,1].\n",
        "        img2 (torch.Tensor): Second image (N, C, H, W) or (C, H, W) in range [0,1].\n",
        "        data_range (float): The range of the data (e.g., 1.0 for [0,1], 255 for [0,255]).\n",
        "        multichannel (bool): Set to True if images have multiple channels.\n",
        "    Returns:\n",
        "        float: SSIM score.\n",
        "    \"\"\"\n",
        "    # SSIM expects numpy arrays and in the correct range\n",
        "    img1_np = img1.squeeze(0).cpu().numpy() # Remove batch dim, move to CPU, convert to numpy\n",
        "    img2_np = img2.squeeze(0).cpu().numpy()\n",
        "\n",
        "    # Handle single channel images (remove channel dimension if it's 1 for SSIM)\n",
        "    if img1_np.ndim == 3 and img1_np.shape[0] == 1:\n",
        "        img1_np = img1_np[0]\n",
        "        multichannel = False # Override if it's actually single channel\n",
        "    if img2_np.ndim == 3 and img2_np.shape[0] == 1:\n",
        "        img2_np = img2_np[0]\n",
        "        multichannel = False\n",
        "\n",
        "    # Transpose to (H, W, C) if it's (C, H, W) for multichannel\n",
        "    if multichannel and img1_np.ndim == 3 and img1_np.shape[0] > 1:\n",
        "        img1_np = np.transpose(img1_np, (1, 2, 0))\n",
        "        img2_np = np.transpose(img2_np, (1, 2, 0))\n",
        "    elif multichannel and img1_np.ndim == 2: # If it's 2D, it's not multichannel\n",
        "        multichannel = False\n",
        "\n",
        "    return ssim(img1_np, img2_np, data_range=data_range, multichannel=multichannel)\n",
        "\n",
        "def calculate_psnr(img1, img2, data_range=1.0):\n",
        "    \"\"\"\n",
        "    Calculates PSNR between two images.\n",
        "    Args:\n",
        "        img1 (torch.Tensor): First image (N, C, H, W) or (C, H, W) in range [0,1].\n",
        "        img2 (torch.Tensor): Second image (N, C, H, W) or (C, H, W) in range [0,1].\n",
        "        data_range (float): The range of the data (e.g., 1.0 for [0,1], 255 for [0,255]).\n",
        "    Returns:\n",
        "        float: PSNR score.\n",
        "    \"\"\"\n",
        "    # PSNR expects numpy arrays and in the correct range\n",
        "    img1_np = img1.squeeze(0).cpu().numpy()\n",
        "    img2_np = img2.squeeze(0).cpu().numpy()\n",
        "\n",
        "    # Handle single channel images (remove channel dimension if it's 1 for PSNR)\n",
        "    if img1_np.ndim == 3 and img1_np.shape[0] == 1:\n",
        "        img1_np = img1_np[0]\n",
        "    if img2_np.ndim == 3 and img2_np.shape[0] == 1:\n",
        "        img2_np = img2_np[0]\n",
        "\n",
        "    return psnr(img1_np, img2_np, data_range=data_range)\n",
        "\n",
        "def calculate_ndvi(eo_image_tensor, eo_band_config):\n",
        "    \"\"\"\n",
        "    Calculates Normalized Difference Vegetation Index (NDVI) from an EO image tensor.\n",
        "    NDVI = (NIR - Red) / (NIR + Red)\n",
        "    Args:\n",
        "        eo_image_tensor (torch.Tensor): EO image tensor (C, H, W) in range [0, 1].\n",
        "                                        Assumes NIR and Red bands are present.\n",
        "        eo_band_config (list): List of band indices used for the EO image.\n",
        "    Returns:\n",
        "        torch.Tensor: NDVI map (1, H, W) in range [-1, 1].\n",
        "                      Returns None if required bands are not present.\n",
        "    \"\"\"\n",
        "    # Find indices of NIR (B8) and Red (B4) in the current EO band configuration\n",
        "    try:\n",
        "        nir_idx = eo_band_config.index(8) # Sentinel-2 B8 is NIR\n",
        "        red_idx = eo_band_config.index(4) # Sentinel-2 B4 is Red\n",
        "    except ValueError:\n",
        "        print(\"Warning: NIR (B8) or Red (B4) band not found in current EO configuration. Cannot calculate NDVI.\")\n",
        "        return None\n",
        "\n",
        "    nir_band = eo_image_tensor[nir_idx, :, :]\n",
        "    red_band = eo_image_tensor[red_idx, :, :]\n",
        "\n",
        "    # Avoid division by zero\n",
        "    denominator = nir_band + red_band\n",
        "    # Add a small epsilon to avoid division by zero\n",
        "    epsilon = 1e-6\n",
        "    ndvi = (nir_band - red_band) / (denominator + epsilon)\n",
        "\n",
        "    return ndvi.unsqueeze(0) # Add channel dimension back"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XU5QaPWyvj3P"
      },
      "source": [
        "# Training Funtion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "4-4TWWO7veUF"
      },
      "outputs": [],
      "source": [
        "\n",
        "def train_cyclegan():\n",
        "    \"\"\"\n",
        "    Main function to design and train the CycleGAN model.\n",
        "    \"\"\"\n",
        "    print(f\"Using device: {config.DEVICE}\")\n",
        "    print(f\"Current EO Output Configuration: {config.CURRENT_EO_CONFIG_NAME} (Bands: {config.EO_BAND_CONFIGS[config.CURRENT_EO_CONFIG_NAME]})\")\n",
        "    print(f\"Input Channels (SAR): {config.INPUT_NC}\")\n",
        "    print(f\"Output Channels (EO): {config.OUTPUT_NC}\")\n",
        "\n",
        "    # Initialize Dataset and DataLoader\n",
        "    dataset = Sen12MSDataset(\n",
        "        root_dir=config.data_root_dir,\n",
        "        sar_dir=config.SAR_DIR,\n",
        "        eo_dir=config.EO_DIR,\n",
        "        eo_bands=config.EO_BAND_CONFIGS[config.CURRENT_EO_CONFIG_NAME],\n",
        "        image_size=config.IMAGE_SIZE\n",
        "    )\n",
        "    dataloader = DataLoader(dataset, batch_size=config.BATCH_SIZE, shuffle=True, num_workers=config.NUM_WORKERS)\n",
        "\n",
        "    # Initialize Generators and Discriminators\n",
        "    # G_A: SAR -> EO, G_B: EO -> SAR\n",
        "    G_A = Generator(config.INPUT_NC, config.OUTPUT_NC, config.NGF, config.N_RESNET_BLOCKS).to(config.DEVICE)\n",
        "    G_B = Generator(config.OUTPUT_NC, config.INPUT_NC, config.NGF, config.N_RESNET_BLOCKS).to(config.DEVICE)\n",
        "    # D_A: Discriminates real EO vs fake EO, D_B: Discriminates real SAR vs fake SAR\n",
        "    D_A = Discriminator(config.OUTPUT_NC, config.NDF).to(config.DEVICE)\n",
        "    D_B = Discriminator(config.INPUT_NC, config.NDF).to(config.DEVICE)\n",
        "\n",
        "    # Initialize Optimizers\n",
        "    optimizer_G = optim.Adam(list(G_A.parameters()) + list(G_B.parameters()), lr=config.LR, betas=(config.BETA1, 0.999))\n",
        "    optimizer_D_A = optim.Adam(D_A.parameters(), lr=config.LR, betas=(config.BETA1, 0.999))\n",
        "    optimizer_D_B = optim.Adam(D_B.parameters(), lr=config.LR, betas=(config.BETA1, 0.999))\n",
        "\n",
        "    # Initialize Loss Functions\n",
        "    criterion_GAN = GANLoss(gan_mode='lsgan').to(config.DEVICE)\n",
        "    criterion_cycle = CycleConsistencyLoss(lambda_cycle=config.LAMBDA_CYCLE).to(config.DEVICE)\n",
        "    criterion_identity = IdentityLoss(lambda_identity=config.LAMBDA_IDENTITY).to(config.DEVICE)\n",
        "\n",
        "    # Training Loop\n",
        "    print(\"Starting Training Loop...\")\n",
        "    for epoch in range(config.NUM_EPOCHS):\n",
        "        G_A.train()\n",
        "        G_B.train()\n",
        "        D_A.train()\n",
        "        D_B.train()\n",
        "\n",
        "        for i, (real_sar, real_eo) in enumerate(tqdm(dataloader, desc=f\"Epoch {epoch+1}/{config.NUM_EPOCHS}\")):\n",
        "            real_sar = real_sar.to(config.DEVICE)\n",
        "            real_eo = real_eo.to(config.DEVICE)\n",
        "\n",
        "            # --- Train Generators G_A and G_B ---\n",
        "            optimizer_G.zero_grad()\n",
        "\n",
        "            # Identity loss (helps preserve color composition)\n",
        "            # G_A is SAR -> EO. Identity loss should be G_A(real_eo) compared to real_eo\n",
        "            identity_eo = G_A(real_eo)\n",
        "            loss_identity_A = criterion_identity(identity_eo, real_eo)\n",
        "\n",
        "            # G_B is EO -> SAR. Identity loss should be G_B(real_sar) compared to real_sar\n",
        "            identity_sar = G_B(real_sar)\n",
        "            loss_identity_B = criterion_identity(identity_sar, real_sar)\n",
        "\n",
        "\n",
        "            # GAN loss D_A(G_A(real_sar))\n",
        "            fake_eo = G_A(real_sar)\n",
        "            pred_fake_eo = D_A(fake_eo)\n",
        "            loss_GAN_A = criterion_GAN(pred_fake_eo, True) # G_A wants to fool D_A\n",
        "\n",
        "            # GAN loss D_B(G_B(real_eo))\n",
        "            fake_sar = G_B(real_eo)\n",
        "            pred_fake_sar = D_B(fake_sar)\n",
        "            loss_GAN_B = criterion_GAN(pred_fake_sar, True) # G_B wants to fool D_B\n",
        "\n",
        "            # Cycle consistency loss\n",
        "            # Cycle SAR -> EO -> SAR\n",
        "            cycled_sar = G_B(fake_eo)\n",
        "            loss_cycle_sar = criterion_cycle(cycled_sar, real_sar)\n",
        "            # Cycle EO -> SAR -> EO\n",
        "            cycled_eo = G_A(fake_sar)\n",
        "            loss_cycle_eo = criterion_cycle(cycled_eo, real_eo)\n",
        "\n",
        "            # # Total Generator Loss\n",
        "            loss_G = loss_GAN_A + loss_GAN_B + loss_cycle_sar + loss_cycle_eo + \\\n",
        "                     loss_identity_A + loss_identity_B\n",
        "            loss_G.backward()\n",
        "            optimizer_G.step()\n",
        "\n",
        "            # # --- Train Discriminator D_A (real EO vs fake EO) ---\n",
        "            optimizer_D_A.zero_grad()\n",
        "            # # Real loss\n",
        "            pred_real_eo = D_A(real_eo)\n",
        "            loss_D_A_real = criterion_GAN(pred_real_eo, True)\n",
        "            # # Fake loss (detach fake_eo to stop gradients from flowing to G_A)\n",
        "            pred_fake_eo = D_A(fake_eo.detach())\n",
        "            loss_D_A_fake = criterion_GAN(pred_fake_eo, False)\n",
        "            # Total D_A loss\n",
        "            loss_D_A = (loss_D_A_real + loss_D_A_fake) * 0.5\n",
        "            loss_D_A.backward()\n",
        "            optimizer_D_A.step()\n",
        "\n",
        "            # # --- Train Discriminator D_B (real SAR vs fake SAR) ---\n",
        "            optimizer_D_B.zero_grad()\n",
        "            # # Real loss\n",
        "            pred_real_sar = D_B(real_sar)\n",
        "            loss_D_B_real = criterion_GAN(pred_real_sar, True)\n",
        "            # Fake loss (detach fake_sar to stop gradients from flowing to G_B)\n",
        "            pred_fake_sar = D_B(fake_sar.detach())\n",
        "            loss_D_B_fake = criterion_GAN(pred_fake_sar, False)\n",
        "            # Total D_B loss\n",
        "            loss_D_B = (loss_D_B_real + loss_D_B_fake) * 0.5\n",
        "            loss_D_B.backward()\n",
        "            optimizer_D_B.step()\n",
        "\n",
        "            if i % config.PRINT_FREQ == 0:\n",
        "                tqdm.write(f\"Epoch [{epoch+1}/{config.NUM_EPOCHS}], Step [{i}/{len(dataloader)}]\\n\"\n",
        "                           f\"Loss_G: {loss_G.item():.4f} | Loss_G_GAN_A: {loss_GAN_A.item():.4f} | Loss_G_GAN_B: {loss_GAN_B.item():.4f}\\n\"\n",
        "                           f\"Loss_cycle_SAR: {loss_cycle_sar.item():.4f} | Loss_cycle_EO: {loss_cycle_eo.item():.4f}\\n\"\n",
        "                           f\"Loss_identity_A: {loss_identity_A.item():.4f} | Loss_identity_B: {loss_identity_B.item():.4f}\\n\"\n",
        "                           f\"Loss_D_A: {loss_D_A.item():.4f} | Loss_D_B: {loss_D_B.item():.4f}\")\n",
        "\n",
        "        # --- Save generated images and evaluate metrics at end of epoch ---\n",
        "        G_A.eval()\n",
        "        G_B.eval()\n",
        "        with torch.no_grad():\n",
        "            # Get a batch for visualization and metric calculation\n",
        "            # Use a fixed batch for consistency in visualization\n",
        "            try:\n",
        "                # Try to get a new sample, or reuse the first one if dataloader is exhausted\n",
        "                dataloader_eval = DataLoader(dataset, batch_size=config.BATCH_SIZE, shuffle=False, num_workers=config.NUM_WORKERS)\n",
        "                sample_sar, sample_eo = next(iter(dataloader_eval))\n",
        "            except StopIteration:\n",
        "                 print(\"Warning: Dataloader exhausted during evaluation. This should not happen with shuffle=False.\")\n",
        "                 # If still no data, use dummy tensors or skip evaluation\n",
        "                 if len(dataset) == 0:\n",
        "                     print(\"Error: Dataset is empty. Skipping evaluation.\")\n",
        "                     continue\n",
        "                 # Fallback: Use the first sample if available\n",
        "                 sample_sar, sample_eo = dataset[0]\n",
        "                 sample_sar = sample_sar.unsqueeze(0) # Add batch dimension\n",
        "                 sample_eo = sample_eo.unsqueeze(0) # Add batch dimension\n",
        "\n",
        "\n",
        "            sample_sar = sample_sar.to(config.DEVICE)\n",
        "            sample_eo = sample_eo.to(config.DEVICE)\n",
        "\n",
        "            # Generate fake EO from real SAR\n",
        "            generated_eo = G_A(sample_sar)\n",
        "            # Cycle back to SAR\n",
        "            cycled_sar_from_eo = G_B(generated_eo)\n",
        "\n",
        "            # Generate fake SAR from real EO\n",
        "            generated_sar = G_B(sample_eo)\n",
        "            # Cycle back to EO\n",
        "            cycled_eo_from_sar = G_A(generated_sar)\n",
        "\n",
        "            # Denormalize for saving and metric calculation (from [-1, 1] to [0, 1])\n",
        "            real_sar_display = denormalize_image(sample_sar)\n",
        "            real_eo_display = denormalize_image(sample_eo)\n",
        "            generated_eo_display = denormalize_image(generated_eo)\n",
        "            cycled_sar_display = denormalize_image(cycled_sar_from_eo)\n",
        "            generated_sar_display = denormalize_image(generated_sar)\n",
        "            cycled_eo_display = denormalize_image(cycled_eo_from_sar)\n",
        "\n",
        "            # Save sample images\n",
        "            # Need to adjust save_combined_image or use vutils.save_image directly for single images\n",
        "            # vutils.save_image expects a grid, so create a grid of 1 image or save individually\n",
        "            vutils.save_image(real_sar_display, os.path.join(config.OUTPUT_DIR, f\"epoch_{epoch+1}_real_sar.png\"), normalize=True)\n",
        "            vutils.save_image(real_eo_display, os.path.join(config.OUTPUT_DIR, f\"epoch_{epoch+1}_{config.CURRENT_EO_CONFIG_NAME}_real_eo.png\"), normalize=True)\n",
        "            vutils.save_image(generated_eo_display, os.path.join(config.OUTPUT_DIR, f\"epoch_{epoch+1}_{config.CURRENT_EO_CONFIG_NAME}_generated_eo.png\"), normalize=True)\n",
        "            vutils.save_image(cycled_sar_display, os.path.join(config.OUTPUT_DIR, f\"epoch_{epoch+1}_cycled_sar.png\"), normalize=True)\n",
        "            vutils.save_image(generated_sar_display, os.path.join(config.OUTPUT_DIR, f\"epoch_{epoch+1}_generated_sar.png\"), normalize=True)\n",
        "            vutils.save_image(cycled_eo_display, os.path.join(config.OUTPUT_DIR, f\"epoch_{epoch+1}_cycled_eo.png\"), normalize=True)\n",
        "\n",
        "\n",
        "            # --- Calculate Performance Metrics ---\n",
        "            # For SSIM/PSNR, compare generated_eo with real_eo\n",
        "\n",
        "            # SSIM and PSNR for SAR -> EO translation\n",
        "            # Ensure images are (C, H, W) or (H, W) numpy arrays for SSIM/PSNR\n",
        "            ssim_score = calculate_ssim(generated_eo_display[0], real_eo_display[0], multichannel=(config.OUTPUT_NC > 1))\n",
        "            psnr_score = calculate_psnr(generated_eo_display[0], real_eo_display[0])\n",
        "            print(f\"Epoch {epoch+1} Metrics (SAR -> EO):\")\n",
        "            print(f\"  SSIM: {ssim_score:.4f}\")\n",
        "            print(f\"  PSNR: {psnr_score:.4f}\")\n",
        "\n",
        "            # NDVI calculation for generated EO and real EO\n",
        "            # NDVI requires NIR (B8) and Red (B4) bands\n",
        "            if 8 in config.EO_BAND_CONFIGS[config.CURRENT_EO_CONFIG_NAME] and \\\n",
        "               4 in config.EO_BAND_CONFIGS[config.CURRENT_EO_CONFIG_NAME]:\n",
        "\n",
        "                # Calculate NDVI for real EO (take the first image in the batch)\n",
        "                real_ndvi = calculate_ndvi(real_eo_display[0], config.EO_BAND_CONFIGS[config.CURRENT_EO_CONFIG_NAME])\n",
        "                # Calculate NDVI for generated EO (take the first image in the batch)\n",
        "                generated_ndvi = calculate_ndvi(generated_eo_display[0], config.EO_BAND_CONFIGS[config.CURRENT_EO_CONFIG_NAME])\n",
        "\n",
        "                if real_ndvi is not None and generated_ndvi is not None:\n",
        "                    # Save NDVI maps visualize as grayscale images\n",
        "                    vutils.save_image(real_ndvi, os.path.join(config.OUTPUT_DIR, f\"epoch_{epoch+1}_real_ndvi.png\"), normalize=True)\n",
        "                    vutils.save_image(generated_ndvi, os.path.join(config.OUTPUT_DIR, f\"epoch_{epoch+1}_generated_ndvi.png\"), normalize=True)\n",
        "\n",
        "\n",
        "                    ndvi_ssim = calculate_ssim(generated_ndvi[0], real_ndvi[0], data_range=2.0, multichannel=False) # NDVI range is [-1, 1]\n",
        "                    ndvi_psnr = calculate_psnr(generated_ndvi[0], real_ndvi[0], data_range=2.0)\n",
        "                    print(f\"  NDVI SSIM: {ndvi_ssim:.4f}\")\n",
        "                    print(f\"  NDVI PSNR: {ndvi_psnr:.4f}\")\n",
        "            else:\n",
        "                print(\"  NDVI not calculated: Required NIR (B8) or Red (B4) band missing in current EO configuration.\")\n",
        "\n",
        "\n",
        "        # Save model checkpoints\n",
        "        if (epoch + 1) % config.SAVE_EPOCH_FREQ == 0:\n",
        "            # Checkpoint directory is already set to Drive path in Config\n",
        "            torch.save(G_A.state_dict(), os.path.join(config.CHECKPOINT_DIR, f'G_A_epoch_{epoch+1}.pth'))\n",
        "            torch.save(G_B.state_dict(), os.path.join(config.CHECKPOINT_DIR, f'G_B_epoch_{epoch+1}.pth'))\n",
        "            torch.save(D_A.state_dict(), os.path.join(config.CHECKPOINT_DIR, f'D_A_epoch_{epoch+1}.pth'))\n",
        "            torch.save(D_B.state_dict(), os.path.join(config.CHECKPOINT_DIR, f'D_B_epoch_{epoch+1}.pth'))\n",
        "            print(f\"Models saved after epoch {epoch+1}\")\n",
        "\n",
        "    print(\"Training Complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sh6EFsQSxgu1"
      },
      "source": [
        "# To Run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "teQJ8kSwxTgI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f63c1901-4fa8-4817-d647-cf9978b2ccf0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n",
            "Current EO Output Configuration: RGB (Bands: [4, 3, 2])\n",
            "Input Channels (SAR): 3\n",
            "Output Channels (EO): 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Checking SAR readability: 100%|██████████| 1947/1947 [00:13<00:00, 141.10it/s]\n",
            "Checking EO readability: 100%|██████████| 1951/1951 [00:29<00:00, 66.33it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 1947 matched SAR-EO pairs.\n",
            "Starting Training Loop...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 1/5:   0%|          | 0/1947 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: SAR image ROIs2017_winter_s1_8_p128.tif has 2 channels, but INPUT_NC is 3. Padding with zeros.\n",
            "Warning: SAR image ROIs2017_winter_s1_8_p545.tif has 2 channels, but INPUT_NC is 3. Padding with zeros.\n",
            "Warning: SAR image ROIs2017_winter_s1_8_p715.tif has 2 channels, but INPUT_NC is 3. Padding with zeros.\n",
            "Warning: SAR image ROIs2017_winter_s1_8_p626.tif has 2 channels, but INPUT_NC is 3. Padding with zeros.\n",
            "Warning: SAR image ROIs2017_winter_s1_8_p201.tif has 2 channels, but INPUT_NC is 3. Padding with zeros.\n",
            "Warning: SAR image ROIs2017_winter_s1_8_p433.tif has 2 channels, but INPUT_NC is 3. Padding with zeros.Warning: SAR image ROIs2017_winter_s1_22_p820.tif has 2 channels, but INPUT_NC is 3. Padding with zeros.\n",
            "Warning: SAR image ROIs2017_winter_s1_8_p36.tif has 2 channels, but INPUT_NC is 3. Padding with zeros.\n",
            "\n",
            "Warning: SAR image ROIs2017_winter_s1_21_p357.tif has 2 channels, but INPUT_NC is 3. Padding with zeros.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/5:   0%|          | 1/1947 [00:36<19:55:51, 36.87s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: SAR image ROIs2017_winter_s1_22_p346.tif has 2 channels, but INPUT_NC is 3. Padding with zeros.\n",
            "Epoch [1/5], Step [0/1947]\n",
            "Loss_G: 17.9890 | Loss_G_GAN_A: 1.0083 | Loss_G_GAN_B: 1.2274\n",
            "Loss_cycle_SAR: 5.5331 | Loss_cycle_EO: 4.9577\n",
            "Loss_identity_A: 2.4995 | Loss_identity_B: 2.7630\n",
            "Loss_D_A: 0.5590 | Loss_D_B: 0.6567\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/5:   0%|          | 2/1947 [01:09<18:28:32, 34.20s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: SAR image ROIs2017_winter_s1_21_p162.tif has 2 channels, but INPUT_NC is 3. Padding with zeros.\n",
            "Epoch [1/5], Step [1/1947]\n",
            "Loss_G: 11.6449 | Loss_G_GAN_A: 0.2981 | Loss_G_GAN_B: 0.4195\n",
            "Loss_cycle_SAR: 4.1571 | Loss_cycle_EO: 3.0779\n",
            "Loss_identity_A: 1.6226 | Loss_identity_B: 2.0697\n",
            "Loss_D_A: 0.5592 | Loss_D_B: 0.6433\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/5:   0%|          | 3/1947 [01:41<17:54:11, 33.15s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: SAR image ROIs2017_winter_s1_21_p181.tif has 2 channels, but INPUT_NC is 3. Padding with zeros.\n",
            "Epoch [1/5], Step [2/1947]\n",
            "Loss_G: 8.4642 | Loss_G_GAN_A: 0.5459 | Loss_G_GAN_B: 0.2512\n",
            "Loss_cycle_SAR: 3.3434 | Loss_cycle_EO: 1.7814\n",
            "Loss_identity_A: 0.8884 | Loss_identity_B: 1.6538\n",
            "Loss_D_A: 0.4227 | Loss_D_B: 1.0588\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/5:   0%|          | 4/1947 [02:13<17:42:16, 32.80s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: SAR image ROIs2017_winter_s1_22_p218.tif has 2 channels, but INPUT_NC is 3. Padding with zeros.\n",
            "Epoch [1/5], Step [3/1947]\n",
            "Loss_G: 8.7172 | Loss_G_GAN_A: 0.9198 | Loss_G_GAN_B: 0.8910\n",
            "Loss_cycle_SAR: 2.7858 | Loss_cycle_EO: 1.7975\n",
            "Loss_identity_A: 0.9317 | Loss_identity_B: 1.3914\n",
            "Loss_D_A: 0.2335 | Loss_D_B: 0.7890\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/5:   0%|          | 5/1947 [02:46<17:42:00, 32.81s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: SAR image ROIs2017_winter_s1_22_p831.tif has 2 channels, but INPUT_NC is 3. Padding with zeros.\n",
            "Epoch [1/5], Step [4/1947]\n",
            "Loss_G: 9.0904 | Loss_G_GAN_A: 0.8261 | Loss_G_GAN_B: 1.6111\n",
            "Loss_cycle_SAR: 2.7553 | Loss_cycle_EO: 1.6926\n",
            "Loss_identity_A: 0.8410 | Loss_identity_B: 1.3644\n",
            "Loss_D_A: 0.1267 | Loss_D_B: 0.8487\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/5:   0%|          | 6/1947 [03:18<17:31:19, 32.50s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: SAR image ROIs2017_winter_s1_21_p740.tif has 2 channels, but INPUT_NC is 3. Padding with zeros.\n",
            "Epoch [1/5], Step [5/1947]\n",
            "Loss_G: 8.6017 | Loss_G_GAN_A: 0.7826 | Loss_G_GAN_B: 0.9054\n",
            "Loss_cycle_SAR: 2.6913 | Loss_cycle_EO: 1.9406\n",
            "Loss_identity_A: 0.9441 | Loss_identity_B: 1.3378\n",
            "Loss_D_A: 0.1094 | Loss_D_B: 0.5005\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/5:   0%|          | 7/1947 [03:49<17:20:37, 32.18s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: SAR image ROIs2017_winter_s1_8_p741.tif has 2 channels, but INPUT_NC is 3. Padding with zeros.\n",
            "Epoch [1/5], Step [6/1947]\n",
            "Loss_G: 7.9329 | Loss_G_GAN_A: 0.8109 | Loss_G_GAN_B: 0.9703\n",
            "Loss_cycle_SAR: 2.1446 | Loss_cycle_EO: 1.9735\n",
            "Loss_identity_A: 0.9848 | Loss_identity_B: 1.0488\n",
            "Loss_D_A: 0.0784 | Loss_D_B: 0.5191\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/5:   0%|          | 8/1947 [04:21<17:17:37, 32.11s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: SAR image ROIs2017_winter_s1_22_p774.tif has 2 channels, but INPUT_NC is 3. Padding with zeros.\n",
            "Epoch [1/5], Step [7/1947]\n",
            "Loss_G: 6.3770 | Loss_G_GAN_A: 1.0017 | Loss_G_GAN_B: 0.4638\n",
            "Loss_cycle_SAR: 2.2514 | Loss_cycle_EO: 1.0239\n",
            "Loss_identity_A: 0.5330 | Loss_identity_B: 1.1032\n",
            "Loss_D_A: 0.0750 | Loss_D_B: 0.3071\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/5:   0%|          | 9/1947 [04:54<17:28:24, 32.46s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: SAR image ROIs2017_winter_s1_22_p631.tif has 2 channels, but INPUT_NC is 3. Padding with zeros.\n",
            "Epoch [1/5], Step [8/1947]\n",
            "Loss_G: 6.5863 | Loss_G_GAN_A: 0.9694 | Loss_G_GAN_B: 0.4355\n",
            "Loss_cycle_SAR: 2.5175 | Loss_cycle_EO: 0.9453\n",
            "Loss_identity_A: 0.4702 | Loss_identity_B: 1.2485\n",
            "Loss_D_A: 0.0859 | Loss_D_B: 0.3058\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/5:   1%|          | 10/1947 [05:26<17:22:08, 32.28s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: SAR image ROIs2017_winter_s1_8_p110.tif has 2 channels, but INPUT_NC is 3. Padding with zeros.\n",
            "Epoch [1/5], Step [9/1947]\n",
            "Loss_G: 6.1479 | Loss_G_GAN_A: 0.8003 | Loss_G_GAN_B: 0.4340\n",
            "Loss_cycle_SAR: 2.4031 | Loss_cycle_EO: 0.8330\n",
            "Loss_identity_A: 0.4950 | Loss_identity_B: 1.1825\n",
            "Loss_D_A: 0.0910 | Loss_D_B: 0.2678\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/5:   1%|          | 11/1947 [05:58<17:14:45, 32.07s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: SAR image ROIs2017_winter_s1_21_p224.tif has 2 channels, but INPUT_NC is 3. Padding with zeros.\n",
            "Epoch [1/5], Step [10/1947]\n",
            "Loss_G: 7.4798 | Loss_G_GAN_A: 0.7812 | Loss_G_GAN_B: 0.4943\n",
            "Loss_cycle_SAR: 1.7567 | Loss_cycle_EO: 2.3935\n",
            "Loss_identity_A: 1.1934 | Loss_identity_B: 0.8608\n",
            "Loss_D_A: 0.1035 | Loss_D_B: 0.2634\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/5:   1%|          | 12/1947 [06:30<17:16:42, 32.15s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: SAR image ROIs2017_winter_s1_22_p819.tif has 2 channels, but INPUT_NC is 3. Padding with zeros.\n",
            "Epoch [1/5], Step [11/1947]\n",
            "Loss_G: 7.8608 | Loss_G_GAN_A: 1.0238 | Loss_G_GAN_B: 0.4439\n",
            "Loss_cycle_SAR: 1.7408 | Loss_cycle_EO: 2.5406\n",
            "Loss_identity_A: 1.2633 | Loss_identity_B: 0.8484\n",
            "Loss_D_A: 0.0731 | Loss_D_B: 0.2515\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/5:   1%|          | 13/1947 [07:03<17:21:21, 32.31s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: SAR image ROIs2017_winter_s1_8_p648.tif has 2 channels, but INPUT_NC is 3. Padding with zeros.\n",
            "Epoch [1/5], Step [12/1947]\n",
            "Loss_G: 8.0988 | Loss_G_GAN_A: 1.0283 | Loss_G_GAN_B: 0.3867\n",
            "Loss_cycle_SAR: 2.0103 | Loss_cycle_EO: 2.4617\n",
            "Loss_identity_A: 1.2318 | Loss_identity_B: 0.9800\n",
            "Loss_D_A: 0.0362 | Loss_D_B: 0.2290\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/5:   1%|          | 14/1947 [07:34<17:13:09, 32.07s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: SAR image ROIs2017_winter_s1_8_p341.tif has 2 channels, but INPUT_NC is 3. Padding with zeros.\n",
            "Epoch [1/5], Step [13/1947]\n",
            "Loss_G: 5.1260 | Loss_G_GAN_A: 0.8834 | Loss_G_GAN_B: 0.3967\n",
            "Loss_cycle_SAR: 1.7269 | Loss_cycle_EO: 0.8601\n",
            "Loss_identity_A: 0.4293 | Loss_identity_B: 0.8296\n",
            "Loss_D_A: 0.0420 | Loss_D_B: 0.2295\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/5:   1%|          | 15/1947 [08:06<17:05:37, 31.85s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: SAR image ROIs2017_winter_s1_22_p461.tif has 2 channels, but INPUT_NC is 3. Padding with zeros.\n",
            "Epoch [1/5], Step [14/1947]\n",
            "Loss_G: 4.9880 | Loss_G_GAN_A: 1.0005 | Loss_G_GAN_B: 0.4175\n",
            "Loss_cycle_SAR: 1.7637 | Loss_cycle_EO: 0.6259\n",
            "Loss_identity_A: 0.3107 | Loss_identity_B: 0.8697\n",
            "Loss_D_A: 0.0457 | Loss_D_B: 0.1998\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/5:   1%|          | 16/1947 [08:37<16:59:46, 31.69s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: SAR image ROIs2017_winter_s1_22_p607.tif has 2 channels, but INPUT_NC is 3. Padding with zeros.\n",
            "Epoch [1/5], Step [15/1947]\n",
            "Loss_G: 5.5379 | Loss_G_GAN_A: 0.6813 | Loss_G_GAN_B: 0.4665\n",
            "Loss_cycle_SAR: 1.9697 | Loss_cycle_EO: 0.9858\n",
            "Loss_identity_A: 0.4710 | Loss_identity_B: 0.9636\n",
            "Loss_D_A: 0.0833 | Loss_D_B: 0.1851\n"
          ]
        }
      ],
      "source": [
        "if __name__ == '__main__':\n",
        "    train_cyclegan()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nDz7M1qBxiXt"
      },
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WVWcdbsUpzcG"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "mount_file_id": "1atBwz4DUxQaR3-9DfpXynSAOtv27iQLp",
      "authorship_tag": "ABX9TyOShQNdGU6bb8ZuvmUipmQ0",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}