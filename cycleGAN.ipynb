{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1atBwz4DUxQaR3-9DfpXynSAOtv27iQLp",
      "authorship_tag": "ABX9TyML9SiFyZig/PtKmYKQEzWe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Disha-Sikka/SAR-to-EO-CycleGAN/blob/main/cycleGAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5O0WzZ17LDUv"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Copying Extracted files in Colab's Local Disk"
      ],
      "metadata": {
        "id": "acjJOSnyex6Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "import os\n",
        "\n",
        "# store location of files in drive. So, that we can copy them\n",
        "drive_path_s1 = '/content/drive/MyDrive/CycleGAN/winter_s1'\n",
        "drive_path_s2 = '/content/drive/MyDrive/CycleGAN/winter_s2'\n",
        "\n",
        "# store location of colab's paths. Where you want to copy files\n",
        "colab_path_s1 = '/content/ROIs2017_winter_s1.tar.gz'\n",
        "colab_path_s2 = '/content/ROIs2017_winter_s2.tar.gz'\n",
        "\n",
        "print(\"Copying ROIs2017_winter_s1.tar.gz from Drive to Colab local disk...\")\n",
        "if os.path.exists(drive_path_s1):\n",
        "    shutil.copytree(drive_path_s1, colab_path_s1) # copytree --> is used to copy a folder while copy is used to cop a zip file\n",
        "    print(\"S1 file copied.\")\n",
        "else:\n",
        "    print(\"Wrong Path\")\n",
        "\n",
        "print(\"Copying ROIs2017_winter_s2.tar.gz from Drive to Colab local disk...\")\n",
        "if os.path.exists(drive_path_s2):\n",
        "    shutil.copytree(drive_path_s2, colab_path_s2)\n",
        "    print(\"S2 file copied.\")\n",
        "else:\n",
        "    print(\"Wrong Path\")\n",
        "\n",
        "print(\"Copying complete.\")"
      ],
      "metadata": {
        "id": "q1CywYH9ahp-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importing Libraries"
      ],
      "metadata": {
        "id": "lt6EuG5EpP59"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "import random\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.utils as vutils\n",
        "\n",
        "from skimage.metrics import structural_similarity as ssim\n",
        "from skimage.metrics import peak_signal_noise_ratio as psnr"
      ],
      "metadata": {
        "id": "R0gKf_UlpM8i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Configuration\n",
        "\n",
        "- For defining the parameters. So, that we can easily change them when we want."
      ],
      "metadata": {
        "id": "vxeNC3Lqucof"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Config:\n",
        "    # Data paths\n",
        "    SAR_DIR = '/content/ROIs2017_winter_s1.tar.gz'\n",
        "    EO_DIR = '/content/ROIs2017_winter_s2.tar.gz'\n",
        "\n",
        "    # Model parameters\n",
        "    INPUT_NC = 2 # Number of input channels for SAR (Sentinel-1 GRD usually has 2: VV, VH)\n",
        "    NGF = 64 # Number of generator filters in the first conv layer\n",
        "    NDF = 64 # Number of discriminator filters in the first conv layer\n",
        "    N_RESNET_BLOCKS = 6 # Number of ResNet blocks in the generator\n",
        "\n",
        "    # Training parameters\n",
        "    BATCH_SIZE = 1 # CycleGAN typically uses batch size 1\n",
        "    NUM_EPOCHS = 20\n",
        "    LR = 0.0002 # Learning rate\n",
        "    BETA1 = 0.5 # Adam optimizer beta1\n",
        "    LAMBDA_CYCLE = 10.0 # Weight for cycle consistency loss\n",
        "    LAMBDA_IDENTITY = 5.0 # Weight for identity mapping loss helps stabilize\n",
        "\n",
        "    # Image parameters\n",
        "    IMAGE_SIZE = 256\n",
        "    NUM_WORKERS = 4\n",
        "\n",
        "    # Output and logging\n",
        "    # Save outputs and checkpoints to Google Drive for persistence across sessions\n",
        "    OUTPUT_BASE_DIR = '/content/drive/MyDrive/CycleGAN/SAR_EO_Project_Outputs' # Base directory in Drive\n",
        "    OUTPUT_DIR = os.path.join(OUTPUT_BASE_DIR, 'output_cyclegan') # Specific output for images\n",
        "    CHECKPOINT_DIR = os.path.join(OUTPUT_BASE_DIR, 'checkpoints_cyclegan') # Specific output for models\n",
        "\n",
        "    SAVE_EPOCH_FREQ = 5 # Save model checkpoints every N epochs\n",
        "    PRINT_FREQ = 1 # Print training loss every N batches\n",
        "\n",
        "    # Device\n",
        "    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    # EO Band Configurations (Sentinel-2 bands)\n",
        "    # B1 (Coastal Aerosol), B2 (Blue), B3 (Green), B4 (Red), B5 (Red Edge 1),\n",
        "    # B6 (Red Edge 2), B7 (Red Edge 3), B8 (NIR), B8A (NIR Narrow), B9 (Water Vapour),\n",
        "    # B10 (SWIR - Cirrus), B11 (SWIR 1), B12 (SWIR 2)\n",
        "    EO_BAND_CONFIGS = {\n",
        "        \"RGB\": [4, 3, 2], # B4, B3, B2 (Red, Green, Blue)\n",
        "        \"NIR_SWIR_RedEdge\": [8, 11, 5], # B8, B11, B5 (NIR, SWIR1, Red Edge 1)\n",
        "        \"RGB_NIR\": [4, 3, 2, 8] # B4, B3, B2, B8 (Red, Green, Blue, NIR)\n",
        "    }\n",
        "\n",
        "    CURRENT_EO_CONFIG_NAME = \"RGB\"\n",
        "    OUTPUT_NC = len(EO_BAND_CONFIGS[CURRENT_EO_CONFIG_NAME])\n",
        "\n",
        "\n",
        "# Initialize configuration\n",
        "config = Config()\n",
        "\n",
        "# Create output directories if they don't exist in Google Drive\n",
        "os.makedirs(config.OUTPUT_DIR, exist_ok=True)\n",
        "os.makedirs(config.CHECKPOINT_DIR, exist_ok=True)\n"
      ],
      "metadata": {
        "id": "Ga39ss3sphLn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = Config()\n",
        "\n",
        "# Create output directories if they don't exist in Google Drive\n",
        "os.makedirs(config.OUTPUT_DIR, exist_ok=True)\n",
        "os.makedirs(config.CHECKPOINT_DIR, exist_ok=True)\n"
      ],
      "metadata": {
        "id": "Hx_jTwjxwKvs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DataLoader and Preprocessing Custom Class\n"
      ],
      "metadata": {
        "id": "scEuwapdmkoy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Sen12MSDataset(Dataset):\n",
        "    def __init__(self, sar_dir, eo_dir, eo_bands, image_size=256):\n",
        "        self.sar_root = os.path.join(sar_dir)\n",
        "        self.eo_root = os.path.join(eo_dir)\n",
        "        self.eo_bands = eo_bands # List of band indices (1-indexed from original paper)\n",
        "        self.image_size = image_size\n",
        "\n",
        "        self.sar_image_paths = sorted(glob.glob(os.path.join(self.sar_root, '**', '*_s1_*.tif'), recursive=True))\n",
        "        # For EO, we need to find the base path for each image pair, then load specific bands\n",
        "        self.eo_base_paths = sorted(glob.glob(os.path.join(self.eo_root, '**', '*_s2_B*.tif'), recursive=True))\n",
        "\n",
        "        self.eo_image_groups = self._group_eo_files(self.eo_base_paths)\n",
        "\n",
        "        # to match SAR to EO\n",
        "        self.pairs = self._match_sar_eo_pairs()\n",
        "\n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.Resize(image_size, interpolation=transforms.InterpolationMode.BICUBIC),\n",
        "            transforms.ToTensor(), # Converts to [0, 1]\n",
        "        ])\n",
        "\n",
        "        print(f\"Found {len(self.pairs)} matched SAR-EO pairs.\")\n",
        "        if len(self.pairs) == 0:\n",
        "            print(\"WARNING: No SAR-EO pairs found. Please check your data paths and extraction.\")\n",
        "            print(f\"SAR root: {self.sar_root}\")\n",
        "            print(f\"EO root: {self.eo_root}\")\n",
        "            print(f\"Example SAR path search: {os.path.join(self.sar_root, '**', '*_s1_*.tif')}\")\n",
        "            print(f\"Example EO path search: {os.path.join(self.eo_root, '**', '*_s2_B*.tif')}\")\n",
        "\n",
        "\n",
        "    def _group_eo_files(self, eo_paths):\n",
        "        \"\"\"Groups EO band files by their common image ID.\"\"\"\n",
        "        groups = {}\n",
        "        for path in eo_paths:\n",
        "            base_name = '_'.join(os.path.basename(path).split('_')[:-1])\n",
        "            if base_name not in groups:\n",
        "                groups[base_name] = []\n",
        "            groups[base_name].append(path)\n",
        "        return groups\n",
        "\n",
        "    def _match_sar_eo_pairs(self):\n",
        "        \"\"\"\n",
        "        Matches SAR and EO image paths based on their common identifier.\n",
        "        Needs to be done beacause we need this while calculating PSNR, NDVI metrices\n",
        "        \"\"\"\n",
        "        matched_pairs = []\n",
        "\n",
        "        # Created a dictionary of SAR image IDs to their full paths\n",
        "        # SAR example: 'ROIs2017_winter_s1_21_p92.tif'\n",
        "        # We want to extract 'ROIs2017_winter_21_p92'\n",
        "        sar_ids_map = {}\n",
        "        for p in self.sar_image_paths:\n",
        "            base_name = os.path.basename(p)\n",
        "            parts = base_name.rsplit('_', 1) # Split from right once by '_'\n",
        "            if len(parts) > 1 and parts[-1].endswith('.tif'):\n",
        "                clean_id = parts[0].replace('_s1_', '_') # e.g., ROIs2017_winter_21_p92\n",
        "                sar_ids_map[clean_id] = p\n",
        "            else: # Fallback if naming is different\n",
        "                 clean_id = base_name.replace('.tif', '').replace('_s1_', '_')\n",
        "                 sar_ids_map[clean_id] = p\n",
        "\n",
        "\n",
        "        for eo_id_raw, eo_band_paths in self.eo_image_groups.items():\n",
        "            # eo_id_raw example: 'ROIs2017_winter_s2_21_p10_s2'\n",
        "            # We want to extract 'ROIs2017_winter_21_p10'\n",
        "            clean_eo_id = eo_id_raw.replace('_s2', '').replace('_s2_', '_') # e.g., ROIs2017_winter_21_p10\n",
        "\n",
        "            if clean_eo_id in sar_ids_map:\n",
        "                matched_pairs.append((sar_ids_map[clean_eo_id], eo_band_paths))\n",
        "            else:\n",
        "                print(f\"No matching SAR found for EO ID: {eo_id_raw} (cleaned: {clean_eo_id})\")\n",
        "\n",
        "        return matched_pairs\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.pairs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sar_path, eo_band_paths = self.pairs[idx]\n",
        "\n",
        "        sar_image_pil = Image.open(sar_path) # loads image\n",
        "\n",
        "        # Convert to numpy array to handle channels if PIL doesn't load it as multi-channel directly\n",
        "        sar_image_np = np.array(sar_image_pil)\n",
        "\n",
        "        # If SAR is grayscale (H, W), convert to (H, W, 1) then to (1, H, W)\n",
        "        if sar_image_np.ndim == 2:\n",
        "            sar_image_np = sar_image_np[:, :, np.newaxis] # Add channel dim (H, W, 1)\n",
        "\n",
        "\n",
        "        if sar_image_np.dtype == np.uint16:\n",
        "            sar_image_tensor = torch.from_numpy(sar_image_np.astype(np.float32)) / 65535.0 # Scale to [0, 1]\n",
        "        else:\n",
        "            sar_image_tensor = torch.from_numpy(sar_image_np).float()\n",
        "\n",
        "        sar_image_tensor = torch.nan_to_num(sar_image_tensor, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "\n",
        "        if sar_image_tensor.ndim == 3 and sar_image_tensor.shape[0] not in [1, 2]: # If first dim is not channel count\n",
        "             sar_image_tensor = sar_image_tensor.permute(2, 0, 1) # Assuming (H, W, C) to (C, H, W)\n",
        "\n",
        "        # Normalize SAR to [-1, 1] (after initial [0,1] scaling from uint16 or float conversion)\n",
        "        # Re-normalize to [-1, 1] based on the current tensor's min/max for robustness\n",
        "        sar_min = sar_image_tensor.min()\n",
        "        sar_max = sar_image_tensor.max()\n",
        "        if sar_max > sar_min:\n",
        "            sar_image_tensor = (sar_image_tensor - sar_min) / (sar_max - sar_min) # Scale to [0, 1]\n",
        "        else:\n",
        "            sar_image_tensor = torch.zeros_like(sar_image_tensor)\n",
        "        sar_image_tensor = sar_image_tensor * 2.0 - 1.0 # Scale to [-1, 1]\n",
        "\n",
        "        # Resize SAR image\n",
        "        sar_image_tensor = transforms.Resize(self.image_size, interpolation=transforms.InterpolationMode.BICUBIC)(sar_image_tensor)\n",
        "\n",
        "\n",
        "        eo_images_list = []\n",
        "        sorted_eo_band_paths = sorted(eo_band_paths, key=lambda x: int(os.path.basename(x).split('_B')[-1].split('.')[0]))\n",
        "\n",
        "        for band_idx in self.eo_bands:\n",
        "            band_path = next((p for p in sorted_eo_band_paths if f'_B{band_idx}.tif' in p), None)\n",
        "            if band_path is None:\n",
        "                print(f\"Warning: Band B{band_idx} not found for EO image group {os.path.basename(os.path.dirname(sar_path))}. Filling with zeros.\")\n",
        "                dummy_array = np.zeros((self.image_size, self.image_size), dtype=np.uint16)\n",
        "                eo_band_img = Image.fromarray(dummy_array)\n",
        "            else:\n",
        "                eo_band_img = Image.open(band_path).convert('I') # 'I' for 32-bit signed integer pixels\n",
        "\n",
        "            eo_images_list.append(self.transform(eo_band_img))\n",
        "\n",
        "        eo_image_tensor = torch.cat(eo_images_list, dim=0)\n",
        "\n",
        "        eo_max_val = 10000.0\n",
        "        eo_image_tensor = torch.clamp(eo_image_tensor, 0, eo_max_val)\n",
        "        eo_image_tensor = (eo_image_tensor / eo_max_val) * 2.0 - 1.0\n",
        "\n",
        "        return sar_image_tensor, eo_image_tensor\n"
      ],
      "metadata": {
        "id": "9dlIpfH9ILjW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Helper function for Convolutional Block\n",
        "def conv_block(in_channels, out_channels, kernel_size, stride, padding, use_bias=False, norm_layer=nn.InstanceNorm2d, activation=nn.ReLU(True)):\n",
        "    \"\"\"A convolutional block with Conv2d, Normalization, and Activation.\"\"\"\n",
        "    layers = [\n",
        "        nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, bias=use_bias)\n",
        "    ]\n",
        "    if norm_layer:\n",
        "        layers.append(norm_layer(out_channels))\n",
        "    if activation:\n",
        "        layers.append(activation)\n",
        "    return nn.Sequential(*layers)\n",
        "\n",
        "# Helper function for Transposed Convolutional Block (for upsampling)\n",
        "def deconv_block(in_channels, out_channels, kernel_size, stride, padding, output_padding, use_bias=False, norm_layer=nn.InstanceNorm2d, activation=nn.ReLU(True)):\n",
        "    \"\"\"A transposed convolutional block with ConvTranspose2d, Normalization, and Activation.\"\"\"\n",
        "    layers = [\n",
        "        nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride, padding, output_padding, bias=use_bias)\n",
        "    ]\n",
        "    if norm_layer:\n",
        "        layers.append(norm_layer(out_channels))\n",
        "    if activation:\n",
        "        layers.append(activation)\n",
        "    return nn.Sequential(*layers)\n"
      ],
      "metadata": {
        "id": "ixHuQGmpMSqe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Class ResNet\n"
      ],
      "metadata": {
        "id": "ZAOBjmLjDiRL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ResnetBlock(nn.Module):\n",
        "    def __init__(self, dim, norm_layer=nn.InstanceNorm2d, use_bias=False):\n",
        "        super(ResnetBlock, self).__init__()\n",
        "        self.conv_block = nn.Sequential(\n",
        "            nn.ReflectionPad2d(1),\n",
        "            conv_block(dim, dim, kernel_size=3, stride=1, padding=0, use_bias=use_bias, norm_layer=norm_layer),\n",
        "            nn.Dropout(0.5), # Added dropout for regularization\n",
        "            nn.ReflectionPad2d(1),\n",
        "            conv_block(dim, dim, kernel_size=3, stride=1, padding=0, use_bias=use_bias, norm_layer=norm_layer, activation=None) # No activation after second conv\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.conv_block(x) # Residual connection\n"
      ],
      "metadata": {
        "id": "kQgQyMakAnHY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Class Generator\n",
        "- U-Net Based\n",
        "- Translates images from Domain A to Domain B\n",
        "- In this first we downsample the image, than pass the image through ResNet Block, than we decode the image by upscaling it and adding a conv layer which converts it to RGB format."
      ],
      "metadata": {
        "id": "H4z_qi-BHRcT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Generator(nn.Module):\n",
        "    \"\"\"\n",
        "    Generator model (U-Net based).\n",
        "    Translates images from domain A to domain B.\n",
        "    \"\"\"\n",
        "    def __init__(self, input_nc, output_nc, ngf=64, n_blocks=9, norm_layer=nn.InstanceNorm2d):\n",
        "        super(Generator, self).__init__()\n",
        "        use_bias = norm_layer == nn.InstanceNorm2d\n",
        "\n",
        "        model = [\n",
        "            nn.ReflectionPad2d(3),\n",
        "            conv_block(input_nc, ngf, kernel_size=7, stride=1, padding=0, use_bias=use_bias, norm_layer=norm_layer)\n",
        "        ]\n",
        "\n",
        "        # Downsampling\n",
        "        n_downsampling = 2\n",
        "        for i in range(n_downsampling):\n",
        "            mult = 2 ** i\n",
        "            model += [\n",
        "                conv_block(ngf * mult, ngf * mult * 2, kernel_size=3, stride=2, padding=1, use_bias=use_bias, norm_layer=norm_layer)\n",
        "            ]\n",
        "\n",
        "        # ResNet blocks\n",
        "        mult = 2 ** n_downsampling\n",
        "        for i in range(n_blocks):\n",
        "            model += [ResnetBlock(ngf * mult, norm_layer=norm_layer, use_bias=use_bias)]\n",
        "\n",
        "        # Upsampling\n",
        "        for i in range(n_downsampling):\n",
        "            mult = 2 ** (n_downsampling - i)\n",
        "            model += [\n",
        "                deconv_block(ngf * mult, int(ngf * mult / 2), kernel_size=3, stride=2, padding=1, output_padding=1, use_bias=use_bias, norm_layer=norm_layer)\n",
        "            ]\n",
        "\n",
        "        model += [\n",
        "            nn.ReflectionPad2d(3),\n",
        "            nn.Conv2d(ngf, output_nc, kernel_size=7, padding=0),\n",
        "            nn.Tanh() # Output activation to map to [-1, 1]\n",
        "        ]\n",
        "        self.model = nn.Sequential(*model)\n",
        "\n",
        "    def forward(self, input):\n",
        "        return self.model(input)\n"
      ],
      "metadata": {
        "id": "dCXifUd9HP5n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Discriminator Class\n",
        "\n",
        "- PatchGAN based\n",
        "- Classifies whether the provided image is real or fake\n",
        "- Convulational layer and Normalization while downscaling.\n",
        "- At last layer, Conv layer and Sigmoid"
      ],
      "metadata": {
        "id": "Pq6-LqHnLaVS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Discriminator(nn.Module):\n",
        "    \"\"\"\n",
        "    Discriminator model (PatchGAN based).\n",
        "    Classifies image patches as real or fake.\n",
        "    \"\"\"\n",
        "    def __init__(self, input_nc, ndf=64, n_layers=3, norm_layer=nn.InstanceNorm2d):\n",
        "        super(Discriminator, self).__init__()\n",
        "        use_bias = norm_layer == nn.InstanceNorm2d\n",
        "\n",
        "        kw = 4 # Kernel width/height\n",
        "        padw = 1 # Padding\n",
        "\n",
        "        model = [\n",
        "            nn.Conv2d(input_nc, ndf, kernel_size=kw, stride=2, padding=padw),\n",
        "            nn.LeakyReLU(0.2, True) # Leaky ReLU prevents dying ReLU problems and allow gradients to flow for negative inputs\n",
        "        ]\n",
        "\n",
        "        for i in range(1, n_layers):\n",
        "            mult = 2 ** i\n",
        "            model += [\n",
        "                conv_block(ndf * mult // 2, ndf * mult, kernel_size=kw, stride=2, padding=padw, use_bias=use_bias, norm_layer=norm_layer, activation=nn.LeakyReLU(0.2, True))\n",
        "            ]\n",
        "\n",
        "        mult = 2 ** n_layers\n",
        "        model += [\n",
        "            conv_block(ndf * mult // 2, ndf * mult, kernel_size=kw, stride=1, padding=padw, use_bias=use_bias, norm_layer=norm_layer, activation=nn.LeakyReLU(0.2, True))\n",
        "        ]\n",
        "\n",
        "        model += [\n",
        "            nn.Conv2d(ndf * mult, 1, kernel_size=kw, stride=1, padding=padw) # Output 1-channel prediction map\n",
        "        ]\n",
        "        self.model = nn.Sequential(*model)\n",
        "\n",
        "    def forward(self, input):\n",
        "        return self.model(input)\n"
      ],
      "metadata": {
        "id": "phRrfhwUIXAj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Adversarial Loss Function\n",
        "\n",
        "- It tells how well discriminator is working in distinguishing real and fake.\n",
        "- It also tells how well generator is working in translating the image from one domain to other domain.\n"
      ],
      "metadata": {
        "id": "vhZFraNuUcW1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GANLoss(nn.Module):\n",
        "    \"\"\"\n",
        "    Adversarial Loss calculation of discriminator and generator functions. Loss Calculated using mean square\n",
        "    \"\"\"\n",
        "    def __init__(self, gan_mode='mse', target_real_label=1.0, target_fake_label=0.0):\n",
        "        super(GANLoss, self).__init__()\n",
        "        self.register_buffer('real_label', torch.tensor(target_real_label))\n",
        "        self.register_buffer('fake_label', torch.tensor(target_fake_label))\n",
        "        self.gan_mode = gan_mode\n",
        "        if gan_mode == 'lsgan': # Least Squares GAN\n",
        "            self.loss = nn.MSELoss()\n",
        "        elif gan_mode == 'vanilla': # Standard GAN (Binary Cross Entropy)\n",
        "            self.loss = nn.BCEWithLogitsLoss()\n",
        "        else:\n",
        "            raise NotImplementedError(f'GAN mode {gan_mode} not implemented.')\n",
        "\n",
        "    def get_target_tensor(self, prediction, target_is_real):\n",
        "        \"\"\"Creates label tensors with the same size as the input prediction.\"\"\"\n",
        "        if target_is_real:\n",
        "            target_tensor = self.real_label\n",
        "        else:\n",
        "            target_tensor = self.fake_label\n",
        "        return target_tensor.expand_as(prediction)\n",
        "\n",
        "    def __call__(self, prediction, target_is_real):\n",
        "        target_tensor = self.get_target_tensor(prediction, target_is_real)\n",
        "        return self.loss(prediction, target_tensor)\n"
      ],
      "metadata": {
        "id": "CxqMQN9DM9YJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cycle Consistency Loss\n",
        "\n",
        "- It calculates the loss while translating SAR to EO and than back to SAR\n",
        "- Mean Absolute Error"
      ],
      "metadata": {
        "id": "Qr-TotPiXq_t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CycleConsistencyLoss(nn.Module):\n",
        "    \"\"\"\n",
        "    L1 loss for cycle consistency.\n",
        "    \"\"\"\n",
        "    def __init__(self, lambda_cycle=10.0): # higher the lambda--> higher is the consistency of cycle\n",
        "        super(CycleConsistencyLoss, self).__init__()\n",
        "        self.lambda_cycle = lambda_cycle\n",
        "        self.loss = nn.L1Loss() # because it encourages pixel-wise accuracy\n",
        "\n",
        "    def forward(self, real_image, cycled_image):\n",
        "        return self.loss(real_image, cycled_image) * self.lambda_cycle\n"
      ],
      "metadata": {
        "id": "DUzxHls7W7m1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Identity Loss\n",
        "\n",
        "- Optional\n",
        "- Done for colour preservation\n",
        "- And training Stability\n"
      ],
      "metadata": {
        "id": "zWr4O3VwdZ8Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class IdentityLoss(nn.Module):\n",
        "    \"\"\"\n",
        "    L1 loss for identity mapping.\n",
        "    Encourages generators to preserve color composition when input is already from the target domain.\n",
        "    \"\"\"\n",
        "    def __init__(self, lambda_identity=5.0):\n",
        "        super(IdentityLoss, self).__init__()\n",
        "        self.lambda_identity = lambda_identity\n",
        "        self.loss = nn.L1Loss()\n",
        "\n",
        "    def forward(self, real_image, identity_image):\n",
        "        return self.loss(real_image, identity_image) * self.lambda_identity"
      ],
      "metadata": {
        "id": "SJpSaV8GalYo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Utilities for Postprocessing and Metrics\n",
        "\n",
        "- To convert pixels value from the range [-1,1] to [0,1] or [0,255]\n",
        "- For showing the images SAR and generated EO side by side.\n",
        "- To measure the similarity between two images.\n",
        "- To calculate peak-signal-to-noise (PSNR) ratio.\n",
        "- To calculate NDVI from EO image"
      ],
      "metadata": {
        "id": "gZAMJJrzmV0q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def denormalize_image(tensor):\n",
        "    \"\"\"\n",
        "    Denormalizes a tensor from [-1, 1] to [0, 1].\n",
        "    Args:\n",
        "        tensor (torch.Tensor): Image tensor in range [-1, 1].\n",
        "    Returns:\n",
        "        torch.Tensor: Image tensor in range [0, 1].\n",
        "    \"\"\"\n",
        "    return (tensor + 1) / 2.0\n",
        "\n",
        "def save_combined_image(sar_img, gen_eo_img, real_eo_img, filename):\n",
        "    \"\"\"\n",
        "    Combines Real SAR, Generated EO, and Real EO images side-by-side and saves them.\n",
        "    Assumes inputs are already denormalized to [0, 1].\n",
        "    \"\"\"\n",
        "    if sar_img.shape[1] == 2: # Check channel dimension (N, C, H, W)\n",
        "        sar_img_display = sar_img.mean(dim=1, keepdim=True).repeat(1, 3, 1, 1) # Convert to grayscale 3-channel\n",
        "    else:\n",
        "        sar_img_display = sar_img.repeat(1, 3, 1, 1) if sar_img.shape[1] == 1 else sar_img\n",
        "\n",
        "    # For EO, if it has 4 channels (RGB+NIR), select RGB for display\n",
        "    if gen_eo_img.shape[1] == 4:\n",
        "        gen_eo_img_display = gen_eo_img[:, :3, :, :] # Take first 3 channels (RGB)\n",
        "        real_eo_img_display = real_eo_img[:, :3, :, :]\n",
        "    else:\n",
        "        gen_eo_img_display = gen_eo_img\n",
        "        real_eo_img_display = real_eo_img\n",
        "\n",
        "    # Concatenate images horizontally\n",
        "    combined_image = torch.cat([sar_img_display, gen_eo_img_display, real_eo_img_display], dim=3) # Concatenate along width\n",
        "    vutils.save_image(combined_image, filename, normalize=True, nrow=1) # normalize=True scales each image in the grid to [0,1]ng)\n",
        "\n",
        "def calculate_ssim(img1, img2, data_range=1.0, multichannel=True):\n",
        "    \"\"\"\n",
        "    Calculates SSIM between two images.\n",
        "    Args:\n",
        "        img1 (torch.Tensor): First image (N, C, H, W) or (C, H, W) in range [0,1].\n",
        "        img2 (torch.Tensor): Second image (N, C, H, W) or (C, H, W) in range [0,1].\n",
        "        data_range (float): The range of the data (e.g., 1.0 for [0,1], 255 for [0,255]).\n",
        "        multichannel (bool): Set to True if images have multiple channels.\n",
        "    Returns:\n",
        "        float: SSIM score.\n",
        "    \"\"\"\n",
        "    # SSIM expects numpy arrays and in the correct range\n",
        "    img1_np = img1.squeeze(0).cpu().numpy() # Remove batch dim, move to CPU, convert to numpy\n",
        "    img2_np = img2.squeeze(0).cpu().numpy()\n",
        "\n",
        "    # Handle single channel images (remove channel dimension if it's 1 for SSIM)\n",
        "    if img1_np.ndim == 3 and img1_np.shape[0] == 1:\n",
        "        img1_np = img1_np[0]\n",
        "        multichannel = False # Override if it's actually single channel\n",
        "    if img2_np.ndim == 3 and img2_np.shape[0] == 1:\n",
        "        img2_np = img2_np[0]\n",
        "        multichannel = False\n",
        "\n",
        "    # Transpose to (H, W, C) if it's (C, H, W) for multichannel\n",
        "    if multichannel and img1_np.ndim == 3 and img1_np.shape[0] > 1:\n",
        "        img1_np = np.transpose(img1_np, (1, 2, 0))\n",
        "        img2_np = np.transpose(img2_np, (1, 2, 0))\n",
        "    elif multichannel and img1_np.ndim == 2: # If it's 2D, it's not multichannel\n",
        "        multichannel = False\n",
        "\n",
        "    return ssim(img1_np, img2_np, data_range=data_range, multichannel=multichannel)\n",
        "\n",
        "def calculate_psnr(img1, img2, data_range=1.0):\n",
        "    \"\"\"\n",
        "    Calculates PSNR between two images.\n",
        "    Args:\n",
        "        img1 (torch.Tensor): First image (N, C, H, W) or (C, H, W) in range [0,1].\n",
        "        img2 (torch.Tensor): Second image (N, C, H, W) or (C, H, W) in range [0,1].\n",
        "        data_range (float): The range of the data (e.g., 1.0 for [0,1], 255 for [0,255]).\n",
        "    Returns:\n",
        "        float: PSNR score.\n",
        "    \"\"\"\n",
        "    # PSNR expects numpy arrays and in the correct range\n",
        "    img1_np = img1.squeeze(0).cpu().numpy()\n",
        "    img2_np = img2.squeeze(0).cpu().numpy()\n",
        "\n",
        "    # Handle single channel images (remove channel dimension if it's 1 for PSNR)\n",
        "    if img1_np.ndim == 3 and img1_np.shape[0] == 1:\n",
        "        img1_np = img1_np[0]\n",
        "    if img2_np.ndim == 3 and img2_np.shape[0] == 1:\n",
        "        img2_np = img2_np[0]\n",
        "\n",
        "    return psnr(img1_np, img2_np, data_range=data_range)\n",
        "\n",
        "def calculate_ndvi(eo_image_tensor, eo_band_config):\n",
        "    \"\"\"\n",
        "    Calculates Normalized Difference Vegetation Index (NDVI) from an EO image tensor.\n",
        "    NDVI = (NIR - Red) / (NIR + Red)\n",
        "    Args:\n",
        "        eo_image_tensor (torch.Tensor): EO image tensor (C, H, W) in range [0, 1].\n",
        "                                        Assumes NIR and Red bands are present.\n",
        "        eo_band_config (list): List of band indices used for the EO image.\n",
        "    Returns:\n",
        "        torch.Tensor: NDVI map (1, H, W) in range [-1, 1].\n",
        "                      Returns None if required bands are not present.\n",
        "    \"\"\"\n",
        "    # Find indices of NIR (B8) and Red (B4) in the current EO band configuration\n",
        "    try:\n",
        "        nir_idx = eo_band_config.index(8) # Sentinel-2 B8 is NIR\n",
        "        red_idx = eo_band_config.index(4) # Sentinel-2 B4 is Red\n",
        "    except ValueError:\n",
        "        print(\"Warning: NIR (B8) or Red (B4) band not found in current EO configuration. Cannot calculate NDVI.\")\n",
        "        return None\n",
        "\n",
        "    nir_band = eo_image_tensor[nir_idx, :, :]\n",
        "    red_band = eo_image_tensor[red_idx, :, :]\n",
        "\n",
        "    # Avoid division by zero\n",
        "    denominator = nir_band + red_band\n",
        "    # Add a small epsilon to avoid division by zero\n",
        "    epsilon = 1e-6\n",
        "    ndvi = (nir_band - red_band) / (denominator + epsilon)\n",
        "\n",
        "    return ndvi.unsqueeze(0) # Add channel dimension back"
      ],
      "metadata": {
        "id": "t4EtYp_mmXF2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training Funtion"
      ],
      "metadata": {
        "id": "XU5QaPWyvj3P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_cyclegan():\n",
        "    \"\"\"\n",
        "    Main function to design and train the CycleGAN model.\n",
        "    \"\"\"\n",
        "    print(f\"Using device: {config.DEVICE}\")\n",
        "    print(f\"Current EO Output Configuration: {config.CURRENT_EO_CONFIG_NAME} (Bands: {config.EO_BAND_CONFIGS[config.CURRENT_EO_CONFIG_NAME]})\")\n",
        "    print(f\"Input Channels (SAR): {config.INPUT_NC}\")\n",
        "    print(f\"Output Channels (EO): {config.OUTPUT_NC}\")\n",
        "\n",
        "    # Initialize Dataset and DataLoader\n",
        "    dataset = Sen12MSDataset(\n",
        "        sar_dir=config.SAR_DIR,\n",
        "        eo_dir=config.EO_DIR,\n",
        "        eo_bands=config.EO_BAND_CONFIGS[config.CURRENT_EO_CONFIG_NAME],\n",
        "        image_size=config.IMAGE_SIZE\n",
        "    )\n",
        "    dataloader = DataLoader(dataset, batch_size=config.BATCH_SIZE, shuffle=True, num_workers=config.NUM_WORKERS)\n",
        "\n",
        "    # Initialize Generators and Discriminators\n",
        "    # G_A: SAR -> EO, G_B: EO -> SAR\n",
        "    G_A = Generator(config.INPUT_NC, config.OUTPUT_NC, config.NGF, config.N_RESNET_BLOCKS).to(config.DEVICE)\n",
        "    G_B = Generator(config.OUTPUT_NC, config.INPUT_NC, config.NGF, config.N_RESNET_BLOCKS).to(config.DEVICE)\n",
        "    # D_A: Discriminates real EO vs fake EO, D_B: Discriminates real SAR vs fake SAR\n",
        "    D_A = Discriminator(config.OUTPUT_NC, config.NDF).to(config.DEVICE)\n",
        "    D_B = Discriminator(config.INPUT_NC, config.NDF).to(config.DEVICE)\n",
        "\n",
        "    # Initialize Optimizers\n",
        "    optimizer_G = optim.Adam(list(G_A.parameters()) + list(G_B.parameters()), lr=config.LR, betas=(config.BETA1, 0.999))\n",
        "    optimizer_D_A = optim.Adam(D_A.parameters(), lr=config.LR, betas=(config.BETA1, 0.999))\n",
        "    optimizer_D_B = optim.Adam(D_B.parameters(), lr=config.LR, betas=(config.BETA1, 0.999))\n",
        "\n",
        "    # Initialize Loss Functions\n",
        "    criterion_GAN = GANLoss(gan_mode='lsgan').to(config.DEVICE)\n",
        "    criterion_cycle = CycleConsistencyLoss(lambda_cycle=config.LAMBDA_CYCLE).to(config.DEVICE)\n",
        "    criterion_identity = IdentityLoss(lambda_identity=config.LAMBDA_IDENTITY).to(config.DEVICE)\n",
        "\n",
        "    # Training Loop\n",
        "    print(\"Starting Training Loop...\")\n",
        "    for epoch in range(config.NUM_EPOCHS):\n",
        "        G_A.train()\n",
        "        G_B.train()\n",
        "        D_A.train()\n",
        "        D_B.train()\n",
        "\n",
        "        for i, (real_sar, real_eo) in enumerate(tqdm(dataloader, desc=f\"Epoch {epoch+1}/{config.NUM_EPOCHS}\")):\n",
        "            real_sar = real_sar.to(config.DEVICE)\n",
        "            real_eo = real_eo.to(config.DEVICE)\n",
        "\n",
        "            # --- Train Generators G_A and G_B ---\n",
        "            optimizer_G.zero_grad()\n",
        "\n",
        "            # Identity loss (optional, but helps preserve color composition)\n",
        "            # G_A should produce real_eo when given real_eo as input\n",
        "            identity_eo = G_A(real_eo)\n",
        "            loss_identity_A = criterion_identity(identity_eo, real_eo)\n",
        "            # G_B should produce real_sar when given real_sar as input\n",
        "            identity_sar = G_B(real_sar)\n",
        "            loss_identity_B = criterion_identity(identity_sar, real_sar)\n",
        "\n",
        "            # GAN loss D_A(G_A(real_sar))\n",
        "            fake_eo = G_A(real_sar)\n",
        "            pred_fake_eo = D_A(fake_eo)\n",
        "            loss_GAN_A = criterion_GAN(pred_fake_eo, True) # G_A wants to fool D_A\n",
        "\n",
        "            # GAN loss D_B(G_B(real_eo))\n",
        "            fake_sar = G_B(real_eo)\n",
        "            pred_fake_sar = D_B(fake_sar)\n",
        "            loss_GAN_B = criterion_GAN(pred_fake_sar, True) # G_B wants to fool D_B\n",
        "\n",
        "            # Cycle consistency loss\n",
        "            # Cycle SAR -> EO -> SAR\n",
        "            cycled_sar = G_B(fake_eo)\n",
        "            loss_cycle_sar = criterion_cycle(cycled_sar, real_sar)\n",
        "            # Cycle EO -> SAR -> EO\n",
        "            cycled_eo = G_A(fake_sar)\n",
        "            loss_cycle_eo = criterion_cycle(cycled_eo, real_eo)\n",
        "\n",
        "            # Total Generator Loss\n",
        "            loss_G = loss_GAN_A + loss_GAN_B + loss_cycle_sar + loss_cycle_eo + \\\n",
        "                     loss_identity_A + loss_identity_B\n",
        "            loss_G.backward()\n",
        "            optimizer_G.step()\n",
        "\n",
        "            # --- Train Discriminator D_A (real EO vs fake EO) ---\n",
        "            optimizer_D_A.zero_grad()\n",
        "            # Real loss\n",
        "            pred_real_eo = D_A(real_eo)\n",
        "            loss_D_A_real = criterion_GAN(pred_real_eo, True)\n",
        "            # Fake loss (detach fake_eo to stop gradients from flowing to G_A)\n",
        "            pred_fake_eo = D_A(fake_eo.detach())\n",
        "            loss_D_A_fake = criterion_GAN(pred_fake_eo, False)\n",
        "            # Total D_A loss\n",
        "            loss_D_A = (loss_D_A_real + loss_D_A_fake) * 0.5\n",
        "            loss_D_A.backward()\n",
        "            optimizer_D_A.step()\n",
        "\n",
        "            # --- Train Discriminator D_B (real SAR vs fake SAR) ---\n",
        "            optimizer_D_B.zero_grad()\n",
        "            # Real loss\n",
        "            pred_real_sar = D_B(real_sar)\n",
        "            loss_D_B_real = criterion_GAN(pred_real_sar, True)\n",
        "            # Fake loss (detach fake_sar to stop gradients from flowing to G_B)\n",
        "            pred_fake_sar = D_B(fake_sar.detach())\n",
        "            loss_D_B_fake = criterion_GAN(pred_fake_sar, False)\n",
        "            # Total D_B loss\n",
        "            loss_D_B = (loss_D_B_real + loss_D_B_fake) * 0.5\n",
        "            loss_D_B.backward()\n",
        "            optimizer_D_B.step()\n",
        "\n",
        "            if i % config.PRINT_FREQ == 0:\n",
        "                tqdm.write(f\"Epoch [{epoch+1}/{config.NUM_EPOCHS}], Step [{i}/{len(dataloader)}]\\n\"\n",
        "                           f\"Loss_G: {loss_G.item():.4f} | Loss_G_GAN_A: {loss_GAN_A.item():.4f} | Loss_G_GAN_B: {loss_GAN_B.item():.4f}\\n\"\n",
        "                           f\"Loss_cycle_SAR: {loss_cycle_sar.item():.4f} | Loss_cycle_EO: {loss_cycle_eo.item():.4f}\\n\"\n",
        "                           f\"Loss_identity_A: {loss_identity_A.item():.4f} | Loss_identity_B: {loss_identity_B.item():.4f}\\n\"\n",
        "                           f\"Loss_D_A: {loss_D_A.item():.4f} | Loss_D_B: {loss_D_B.item():.4f}\")\n",
        "\n",
        "        # --- Save generated images and evaluate metrics at end of epoch ---\n",
        "        G_A.eval()\n",
        "        G_B.eval()\n",
        "        with torch.no_grad():\n",
        "            # Get a batch for visualization and metric calculation\n",
        "            # Use a fixed batch for consistency in visualization\n",
        "            try:\n",
        "                # Try to get a new sample, or reuse the first one if dataloader is exhausted\n",
        "                sample_sar, sample_eo = next(iter(dataloader))\n",
        "            except StopIteration:\n",
        "                # If dataloader is exhausted, re-initialize it for evaluation\n",
        "                dataloader_eval = DataLoader(dataset, batch_size=config.BATCH_SIZE, shuffle=False, num_workers=config.NUM_WORKERS)\n",
        "                sample_sar, sample_eo = next(iter(dataloader_eval))\n",
        "\n",
        "\n",
        "            sample_sar = sample_sar.to(config.DEVICE)\n",
        "            sample_eo = sample_eo.to(config.DEVICE)\n",
        "\n",
        "            # Generate fake EO from real SAR\n",
        "            generated_eo = G_A(sample_sar)\n",
        "            # Cycle back to SAR\n",
        "            cycled_sar_from_eo = G_B(generated_eo)\n",
        "\n",
        "            # Generate fake SAR from real EO\n",
        "            generated_sar = G_B(sample_eo)\n",
        "            # Cycle back to EO\n",
        "            cycled_eo_from_sar = G_A(generated_sar)\n",
        "\n",
        "            # Denormalize for saving and metric calculation (from [-1, 1] to [0, 1])\n",
        "            real_sar_display = denormalize_image(sample_sar)\n",
        "            real_eo_display = denormalize_image(sample_eo)\n",
        "            generated_eo_display = denormalize_image(generated_eo)\n",
        "            cycled_sar_display = denormalize_image(cycled_sar_from_eo)\n",
        "            generated_sar_display = denormalize_image(generated_sar)\n",
        "            cycled_eo_display = denormalize_image(cycled_eo_from_sar)\n",
        "\n",
        "            # Save sample images\n",
        "            save_combined_image(real_sar_display, os.path.join(config.OUTPUT_DIR, f\"epoch_{epoch+1}_real_sar.png\"))\n",
        "            save_combined_image(real_eo_display, os.path.join(config.OUTPUT_DIR, f\"epoch_{epoch+1}_{config.CURRENT_EO_CONFIG_NAME}_real_eo.png\"))\n",
        "            save_combined_image(generated_eo_display, os.path.join(config.OUTPUT_DIR, f\"epoch_{epoch+1}_{config.CURRENT_EO_CONFIG_NAME}_generated_eo.png\"))\n",
        "            save_combined_image(cycled_sar_display, os.path.join(config.OUTPUT_DIR, f\"epoch_{epoch+1}_cycled_sar.png\"))\n",
        "            save_combined_image(generated_sar_display, os.path.join(config.OUTPUT_DIR, f\"epoch_{epoch+1}_generated_sar.png\"))\n",
        "            save_combined_image(cycled_eo_display, os.path.join(config.OUTPUT_DIR, f\"epoch_{epoch+1}_cycled_eo.png\"))\n",
        "\n",
        "            # --- Calculate Performance Metrics ---\n",
        "            # For SSIM/PSNR, compare generated_eo with real_eo\n",
        "\n",
        "            # SSIM and PSNR for SAR -> EO translation\n",
        "            ssim_score = calculate_ssim(generated_eo_display, real_eo_display, multichannel=(config.OUTPUT_NC > 1))\n",
        "            psnr_score = calculate_psnr(generated_eo_display, real_eo_display)\n",
        "            print(f\"Epoch {epoch+1} Metrics (SAR -> EO):\")\n",
        "            print(f\"  SSIM: {ssim_score:.4f}\")\n",
        "            print(f\"  PSNR: {psnr_score:.4f}\")\n",
        "\n",
        "            # NDVI calculation for generated EO and real EO\n",
        "            # NDVI requires NIR (B8) and Red (B4) bands\n",
        "            if 8 in config.EO_BAND_CONFIGS[config.CURRENT_EO_CONFIG_NAME] and \\\n",
        "               4 in config.EO_BAND_CONFIGS[config.CURRENT_EO_CONFIG_NAME]:\n",
        "\n",
        "                # Calculate NDVI for real EO (take the first image in the batch)\n",
        "                real_ndvi = calculate_ndvi(real_eo_display[0], config.EO_BAND_CONFIGS[config.CURRENT_EO_CONFIG_NAME])\n",
        "                # Calculate NDVI for generated EO (take the first image in the batch)\n",
        "                generated_ndvi = calculate_ndvi(generated_eo_display[0], config.EO_BAND_CONFIGS[config.CURRENT_EO_CONFIG_NAME])\n",
        "\n",
        "                if real_ndvi is not None and generated_ndvi is not None:\n",
        "                    # Save NDVI maps visualize as grayscale images\n",
        "                    save_combined_image(real_ndvi, os.path.join(config.OUTPUT_DIR, f\"epoch_{epoch+1}_real_ndvi.png\"))\n",
        "                    save_combined_image(generated_ndvi, os.path.join(config.OUTPUT_DIR, f\"epoch_{epoch+1}_generated_ndvi.png\"))\n",
        "\n",
        "                    ndvi_ssim = calculate_ssim(generated_ndvi, real_ndvi, data_range=2.0, multichannel=False) # NDVI range is [-1, 1]\n",
        "                    ndvi_psnr = calculate_psnr(generated_ndvi, real_ndvi, data_range=2.0)\n",
        "                    print(f\"  NDVI SSIM: {ndvi_ssim:.4f}\")\n",
        "                    print(f\"  NDVI PSNR: {ndvi_psnr:.4f}\")\n",
        "            else:\n",
        "                print(\"  NDVI not calculated: Required NIR (B8) or Red (B4) band missing in current EO configuration.\")\n",
        "\n",
        "\n",
        "        # Save model checkpoints\n",
        "        if (epoch + 1) % config.SAVE_EPOCH_FREQ == 0:\n",
        "            # Checkpoint directory is already set to Drive path in Config\n",
        "            torch.save(G_A.state_dict(), os.path.join(config.CHECKPOINT_DIR, f'G_A_epoch_{epoch+1}.pth'))\n",
        "            torch.save(G_B.state_dict(), os.path.join(config.CHECKPOINT_DIR, f'G_B_epoch_{epoch+1}.pth'))\n",
        "            torch.save(D_A.state_dict(), os.path.join(config.CHECKPOINT_DIR, f'D_A_epoch_{epoch+1}.pth'))\n",
        "            torch.save(D_B.state_dict(), os.path.join(config.CHECKPOINT_DIR, f'D_B_epoch_{epoch+1}.pth'))\n",
        "            print(f\"Models saved after epoch {epoch+1}\")\n",
        "\n",
        "    print(\"Training Complete!\")\n"
      ],
      "metadata": {
        "id": "4-4TWWO7veUF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# To Run"
      ],
      "metadata": {
        "id": "sh6EFsQSxgu1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    train_cyclegan()\n"
      ],
      "metadata": {
        "id": "teQJ8kSwxTgI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nDz7M1qBxiXt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}